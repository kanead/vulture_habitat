# select swazi data which is the data we tracked in Swaziland
swazi_data <- filter(mydata, study == "swazi")
swazi_data
#' Check for duplicated observations (ones with same lat, long, timestamp,
#'  and individual identifier).
ind2<-swazi_data %>% select(time, long, lat, id) %>%
duplicated
sum(ind2)
# remove them
swazi_data$dups <- ind2
swazi_data <- filter(swazi_data,dups=="FALSE")
swazi_data
# set the time column
levels(factor(swazi_data$id))
# can look at an individual level with
(filter(swazi_data,id=="ID1"))
# all of the data is in the format of day-month-year
swazi_data$New_time<-parse_date_time(x=swazi_data$time,c("%d/%m/%Y %H:%M"))
# keep only the new time data
swazi_data <- select(swazi_data, New_time,long,lat,id,species,study)
swazi_data <- rename(swazi_data, time = New_time)
swazi_data
#' estimate vmax for threshold speed
#' names(swazi_data)[names(swazi_data) == 'time'] <- 'DateTime'
#' speed.est.data <- swazi_data %>% filter(id == "ID2") %>%  select(id,DateTime,lat,long)
#' speed.est.data$qi = 5
#' est.vmax(sdata = data.frame(speed.est.data))
#' filter extreme data based on a speed threshold
#' based on vmax which is km/hr
#' time needs to be labelled DateTime for these functions to work
names(swazi_data)[names(swazi_data) == 'time'] <- 'DateTime'
SDLfilterData<-ddfilter.speed(data.frame(swazi_data), vmax = 70, method = 1)
length(SDLfilterData$DateTime)
#' rename everything as before
swazi_data <- SDLfilterData
names(swazi_data)[names(swazi_data) == 'DateTime'] <- 'time'
#' get the map of the world
m = getMap()
coordinates(swazi_data) = ~long+lat
proj4string(swazi_data) = proj4string(m)
#' Now I can extract any of the columns from the map data corresponding to each point with over:
over(swazi_data,m)$NAME
levels(over(swazi_data,m)$NAME)
#' I can add the NAME to the source points:
swazi_data$country = over(swazi_data,m)$NAME
swazi_data$country <- droplevels(swazi_data$country)
head(swazi_data$country)
countries <- levels(swazi_data$country)
write.csv(countries, file = "results/swazi_countries_overlap.csv", row.names = F)
levels(as.factor(mydata$study))
mydata1$long <- as.numeric(mydata1$long)
mydata1$lat <- as.numeric(mydata1$lat)
mydata <- mydata1 %>% drop_na()
#' filter the data to remove obvious outliers
mydata <- dplyr::filter(mydata1, lat < 20 & lat > -40 & long > 15 & long < 50)
head(mydata)
tail(mydata)
summary(mydata)
str(mydata)
levels(as.factor(mydata$study))
#' How many countries does the bird track overlap with?
#' Swaziland Vulture Tracking Dataset
#' load packages
library(rworldmap)
library(sp)
library(lubridate)
library(SDLfilter)
library(amt)
# swazi
# select swazi data which is the data we tracked in Swaziland
swazi_data <- filter(mydata, study == "swazi")
swazi_data
#' Check for duplicated observations (ones with same lat, long, timestamp,
#'  and individual identifier).
ind2<-swazi_data %>% select(time, long, lat, id) %>%
duplicated
sum(ind2)
# remove them
swazi_data$dups <- ind2
swazi_data <- filter(swazi_data,dups=="FALSE")
swazi_data
# set the time column
levels(factor(swazi_data$id))
# can look at an individual level with
(filter(swazi_data,id=="ID1"))
# all of the data is in the format of day-month-year
swazi_data$New_time<-parse_date_time(x=swazi_data$time,c("%d/%m/%Y %H:%M"))
# keep only the new time data
swazi_data <- select(swazi_data, New_time,long,lat,id,species,study)
swazi_data <- rename(swazi_data, time = New_time)
swazi_data
#' estimate vmax for threshold speed
#' names(swazi_data)[names(swazi_data) == 'time'] <- 'DateTime'
#' speed.est.data <- swazi_data %>% filter(id == "ID2") %>%  select(id,DateTime,lat,long)
#' speed.est.data$qi = 5
#' est.vmax(sdata = data.frame(speed.est.data))
#' filter extreme data based on a speed threshold
#' based on vmax which is km/hr
#' time needs to be labelled DateTime for these functions to work
names(swazi_data)[names(swazi_data) == 'time'] <- 'DateTime'
SDLfilterData<-ddfilter.speed(data.frame(swazi_data), vmax = 70, method = 1)
length(SDLfilterData$DateTime)
#' rename everything as before
swazi_data <- SDLfilterData
names(swazi_data)[names(swazi_data) == 'DateTime'] <- 'time'
#' get the map of the world
m = getMap()
coordinates(swazi_data) = ~long+lat
proj4string(swazi_data) = proj4string(m)
#' Now I can extract any of the columns from the map data corresponding to each point with over:
over(swazi_data,m)$NAME
levels(over(swazi_data,m)$NAME)
#' I can add the NAME to the source points:
swazi_data$country = over(swazi_data,m)$NAME
swazi_data$country <- droplevels(swazi_data$country)
head(swazi_data$country)
countries <- levels(swazi_data$country)
write.csv(countries, file = "results/swazi_countries_overlap.csv", row.names = F)
# Swaziland Vulture Tracking Dataset
library(lubridate)
library(SDLfilter)
library(amt)
library(sp)
# swazi
# select swazi data which is the data we tracked in Swaziland
swazi_data <- filter(mydata, study == "swazi")
swazi_data
#' Check for duplicated observations (ones with same lat, long, timestamp,
#'  and individual identifier).
ind2<-swazi_data %>% select(time, long, lat, id) %>%
duplicated
sum(ind2)
# remove them
swazi_data$dups <- ind2
swazi_data <- filter(swazi_data,dups=="FALSE")
swazi_data
# set the time column
levels(factor(swazi_data$id))
# can look at an individual level with
(filter(swazi_data,id=="ID1"))
# all of the data is in the format of day-month-year
swazi_data$New_time<-parse_date_time(x=swazi_data$time,c("%d/%m/%Y %H:%M"))
# keep only the new time data
swazi_data <- select(swazi_data, New_time,long,lat,id,species,study)
swazi_data <- rename(swazi_data, time = New_time)
swazi_data
#' estimate vmax for threshold speed
#' names(swazi_data)[names(swazi_data) == 'time'] <- 'DateTime'
#' speed.est.data <- swazi_data %>% filter(id == "ID2") %>%  select(id,DateTime,lat,long)
#' speed.est.data$qi = 5
#' est.vmax(sdata = data.frame(speed.est.data))
#' filter extreme data based on a speed threshold
#' based on vmax which is km/hr
#' time needs to be labelled DateTime for these functions to work
names(swazi_data)[names(swazi_data) == 'time'] <- 'DateTime'
SDLfilterData<-ddfilter.speed(data.frame(swazi_data), vmax = 70, method = 1)
length(SDLfilterData$DateTime)
#' rename everything as before
swazi_data <- SDLfilterData
names(swazi_data)[names(swazi_data) == 'DateTime'] <- 'time'
# check the minimum time and the maximum time
min_time <- swazi_data %>% group_by(id) %>% slice(which.min(time))
data.frame(min_time)
max_time <- swazi_data %>% group_by(id) %>% slice(which.max(time))
data.frame(max_time)
#' determine the length of time each bird was tracked for
duration <- difftime(max_time$time, min_time$time, units = "days");duration
#' try the amt package
trk <-
mk_track(
swazi_data,
.x = long,
.y = lat,
.t = time,
id = id,
species = species,
crs = CRS("+init=epsg:4326"))  %>%
transform_coords(
sp::CRS( #' we can transform the CRS of the data to an equal area projection
#' https://epsg.io/102022
"+proj=aea +lat_1=20 +lat_2=-23 +lat_0=0 +lon_0=25 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs"
)
)
#' summarise the sampling rate
data_summary <- trk %>% nest(-id) %>% mutate(sr = map(data, summarize_sampling_rate)) %>%
dplyr::select(id, sr) %>% unnest %>% arrange(id) ; data_summary
#' Calculate home range size for data that is not regularised
mcps <- trk %>% nest(-id) %>%
mutate(mcparea = map(data, ~ hr_mcp(., levels = c(0.95)) %>% hr_area)) %>%
select(id, mcparea) %>% unnest()
mcps$area <- mcps$area / 1000000
mcp_95 <- mcps %>% arrange(id)
#' Same for KDE
kde <- trk %>% nest(-id) %>%
mutate(kdearea = map(data, ~ hr_kde(., levels = c(0.95)) %>% hr_area)) %>%
select(id, kdearea) %>% unnest()
kde$kdearea <-  kde$kdearea / 1000000
kde_95 <- kde %>% arrange(id)
#' combine the summary stats
data_summary$duration <- duration
data_summary$min_time <- min_time$time
data_summary$max_time <- max_time$time
data_summary$kde <- kde_95$kdearea
data_summary$mcps <- mcp_95$area
data_summary$species <- min_time$species
data_summary$study <- "Swazi"
data_summary
#' export the cleaned tracks
customFun  = function(DF) {
write.csv(DF,paste0("",unique(DF$id),".csv"),row.names = FALSE)
return(DF)
}
trk %>%
group_by(id) %>%
do(customFun(.))
#' export the cleaned tracks
#' write the function
customFun  = function(DF) {
write.csv(DF,paste0("",unique(DF$id),".csv"),row.names = FALSE)
return(DF)
}
#' apply the function to the data set by bird ID
swazi_data %>%
group_by(id) %>%
do(customFun(.))
#' write the function
customFun  = function(DF) {
write.csv(DF,paste0("",unique(DF$id),".csv"),row.names = FALSE)
return(DF)
}
#' apply the function to the data set by bird ID
swazi_data %>%
group_by(id) %>%
select(time, long, lat, id, species, study)
do(customFun(.))
#' apply the function to the data set by bird ID
swazi_data %>%
group_by(id) %>%
select(time, long, lat, id, species, study) %>%
do(customFun(.))
#' map the whole data set from 1_load_data
#' load packages
library(sf)
library(sp)
library(hablar)
library(rnaturalearth)
# change the lat long columns to numeric class
mydata <- mydata %>%
convert(num(long:lat))
str(mydata)
# Spatial data with sf
# Create sf object with geo_data data frame and CRS
points_sf <- st_as_sf(mydata, coords = c("long", "lat"), crs = 4326)
class(points_sf)
str(points_sf)
# Get coastal and country world maps as Spatial objects
coast_sp <- ne_coastline(scale = "medium")
countries_sp <- ne_countries(scale = "medium")
# Convert them to sf format
coast_sf <- ne_coastline(scale = "medium", returnclass = "sf")
countries_sf <- ne_countries(scale = "medium", returnclass = "sf")
#' map a sample of the data
Samp <- mydata %>% sample_n(size=100000)
Samp
#' plot(Samp$long, Samp$lat)
#library(ggmap)
#qmplot(long,
#       lat,
#       data = Samp,
#       maptype = "toner-lite")
#' using leaflet
#' Library
library(leaflet)
#' Remove the suspected hybrid species and unknown vul
species_for_removal<-c("cv_wb", "vul")
Samp <- Samp %>% filter(!species %in% species_for_removal) %>% droplevels()
levels(as.factor(Samp$id))
levels(as.factor(Samp$species))
# Call RColorBrewer::display.brewer.all() to see all possible palettes
pal <- colorFactor(
palette = 'Accent',
domain = Samp$species
)
# Final Map
m <- leaflet(Samp) %>%
addTiles()  %>%
# setView( lat=-27, lng=170 , zoom=4) %>%
addProviderTiles("Esri.WorldImagery") %>%
addCircleMarkers(
~ long,
~ lat,
fillOpacity = 0.7,
color = ~ pal(species),
radius = 3,
stroke = FALSE
) %>%
addLegend(
"bottomright",
pal = pal,
values = ~ species,
title = "Species",
opacity = 1
)
m
#' map with just the gyps
not_gyps <-c("hv", "lf", "wh")
gyps <- Samp %>% filter(!species %in% not_gyps) %>% droplevels()
# Final Map
m1 <- leaflet(gyps) %>%
addTiles()  %>%
# setView( lat=-27, lng=170 , zoom=4) %>%
addProviderTiles("Esri.WorldImagery") %>%
addCircleMarkers(
~ long,
~ lat,
fillOpacity = 0.7,
color = ~ pal(species),
radius = 3,
stroke = FALSE
) %>%
addLegend(
"bottomright",
pal = pal,
values = ~ species,
title = "Species",
opacity = 1
)
m1
#' Plot the white back IDs by colour
not_wbs <-c("cv", "rv")
wb <- gyps %>% filter(!species %in% not_wbs) %>% droplevels()
#' need a new palette here
# Call RColorBrewer::display.brewer.all() to see all possible palettes
# pal1 <- colorFactor(viridis(15), domain = wb$study) # I'm assuming the variable ward contains the names of the communities.
# If you want to set your own colors manually: https://www.rapidtables.com/web/color/color-scheme.html
pal1 <- colorFactor(
palette = c('#A2C936', '#C9A736', '#3682C9', '#36B1C9', '#36C9BB',"#36C98E", "#36C94C"
, "#C95336", "#C93636", "#7836C9", "#C9365F", "#4C36C9", "#000000", "#FFFFFF", "#FFA8AD"),
domain = wb$study
)
# Final Map
m2 <- leaflet(wb) %>%
addTiles()  %>%
# setView( lat=-27, lng=170 , zoom=4) %>%
addProviderTiles("Esri.WorldImagery") %>%
addCircleMarkers(
~ long,
~ lat,
fillOpacity = 0.7,
color = ~ pal1(study),
radius = 3,
stroke = FALSE
) %>%
addLegend(
"bottomright",
pal = pal1,
values = ~ study,
title = "Study",
opacity = 1
)
m2
#########################################################################
#' Vulture comparative analysis
#' tutorials here https://www.jessesadler.com/post/gis-with-r-intro/
#' and here https://www.r-spatial.org/
#' 06 November 2018
#' 1_load_data - this loads in all of the tracking data and binds it
#########################################################################
#' Load the required packages
library(readr)
library(tidyverse)
#' Section 1: Load the data ----
data_path <- "raw_data"   # path to the data
files <- dir(data_path, pattern = "*.csv") # get file names
length(files)
mydata1 <- files %>%
# read in all the files, appending the path before the filename
map(~ read_csv(file.path(data_path, .))) %>%
reduce(rbind)
mydata1$long <- as.numeric(mydata1$long)
mydata1$lat <- as.numeric(mydata1$lat)
mydata <- mydata1 %>% drop_na()
#' filter the data to remove obvious outliers
mydata <- dplyr::filter(mydata1, lat < 20 & lat > -40 & long > 15 & long < 50)
head(mydata)
tail(mydata)
summary(mydata)
str(mydata)
levels(as.factor(mydata$study))
#' How many countries does the bird track overlap with?
#' Swaziland Vulture Tracking Dataset
#' load packages
library(rworldmap)
library(sp)
library(lubridate)
library(SDLfilter)
library(amt)
# swazi
# select swazi data which is the data we tracked in Swaziland
swazi_data <- filter(mydata, study == "swazi")
swazi_data
#' Check for duplicated observations (ones with same lat, long, timestamp,
#'  and individual identifier).
ind2<-swazi_data %>% select(time, long, lat, id) %>%
duplicated
sum(ind2)
# remove them
swazi_data$dups <- ind2
swazi_data <- filter(swazi_data,dups=="FALSE")
swazi_data
# set the time column
levels(factor(swazi_data$id))
# can look at an individual level with
(filter(swazi_data,id=="ID1"))
# all of the data is in the format of day-month-year
swazi_data$New_time<-parse_date_time(x=swazi_data$time,c("%d/%m/%Y %H:%M"))
# keep only the new time data
swazi_data <- select(swazi_data, New_time,long,lat,id,species,study)
swazi_data <- rename(swazi_data, time = New_time)
swazi_data
#' estimate vmax for threshold speed
#' names(swazi_data)[names(swazi_data) == 'time'] <- 'DateTime'
#' speed.est.data <- swazi_data %>% filter(id == "ID2") %>%  select(id,DateTime,lat,long)
#' speed.est.data$qi = 5
#' est.vmax(sdata = data.frame(speed.est.data))
#' filter extreme data based on a speed threshold
#' based on vmax which is km/hr
#' time needs to be labelled DateTime for these functions to work
names(swazi_data)[names(swazi_data) == 'time'] <- 'DateTime'
SDLfilterData<-ddfilter.speed(data.frame(swazi_data), vmax = 70, method = 1)
length(SDLfilterData$DateTime)
#' rename everything as before
swazi_data <- SDLfilterData
names(swazi_data)[names(swazi_data) == 'DateTime'] <- 'time'
head(swazi_data)
export_data <- select(swazi_data, time, long, lat, id)
head(export_data)
write.csv(export_data, file = "results/vul_data_niamh.csv", row.names = F)
#########################################################################
#' Vulture comparative analysis
#' tutorials here https://www.jessesadler.com/post/gis-with-r-intro/
#' and here https://www.r-spatial.org/
#' 06 November 2018
#' 1_load_data - this loads in all of the tracking data and binds it
#########################################################################
#' Load the required packages
library(readr)
library(tidyverse)
#' Section 1: Load the data ----
data_path <- "raw_data"   # path to the data
files <- dir(data_path, pattern = "*.csv") # get file names
length(files)
mydata1 <- files %>%
# read in all the files, appending the path before the filename
map(~ read_csv(file.path(data_path, .))) %>%
reduce(rbind)
mydata1$long <- as.numeric(mydata1$long)
mydata1$lat <- as.numeric(mydata1$lat)
mydata <- mydata1 %>% drop_na()
#' filter the data to remove obvious outliers
mydata <- dplyr::filter(mydata1, lat < 20 & lat > -40 & long > 15 & long < 50)
head(mydata)
tail(mydata)
summary(mydata)
str(mydata)
levels(as.factor(mydata$study))
# prep the data
ID = c(rep("A",5), rep("B",5))
DateTime = c("2014-09-25 08:39:45", "2014-09-25 08:39:48", "2014-09-25 08:40:44", "2014-09-25 09:04:00","2014-09-25 09:04:10", "2014-09-25 08:33:32", "2014-09-25 08:34:41", "2014-09-25 08:35:24", "2014-09-25 09:04:00", "2014-09-25 09:04:09")
speed = c(1:10)
df = data.frame(ID,DateTime,speed, stringsAsFactors = FALSE)
df$DateTime<-as.POSIXct(df$DateTime, tz = "UTC")
library(dplyr)
df %>%
group_by(ID) %>%
mutate(timeDiff = c(NA, difftime(tail(DateTime, -1), head(DateTime, -1), units="sec"))) %>%
mutate(newID = paste0(ID, cumsum(!is.na(timeDiff) & timeDiff > 100))) %>%
ungroup()
# prep the data
ID = c(rep("A",5), rep("B",5))
DateTime = c("2014-09-25 08:39:45", "2014-09-25 08:39:48", "2014-09-25 08:40:44", "2014-09-25 09:04:00","2014-09-25 09:04:10", "2014-09-25 08:33:32", "2014-09-25 08:34:41", "2014-09-25 08:35:24", "2014-09-25 09:04:00", "2014-09-25 09:04:09")
speed = c(1:10)
df = data.frame(ID,DateTime,speed, stringsAsFactors = FALSE)
df$DateTime<-as.POSIXct(df$DateTime, tz = "UTC")
library(dplyr)
df %>%
group_by(ID) %>%
mutate(timeDiff = c(NA, difftime(tail(DateTime, -1), head(DateTime, -1), units="sec"))) %>%
mutate(newID = paste(ID, cumsum(!is.na(timeDiff) & timeDiff > 100),sep = "_")) %>%
ungroup()
# prep the data
ID = c(rep("A",5), rep("B",5))
DateTime = c("2014-09-25 08:39:45", "2014-09-25 08:39:48", "2014-09-25 08:40:44", "2014-09-25 09:04:00","2014-09-25 09:04:10", "2014-09-25 08:33:32", "2014-09-25 08:34:41", "2014-09-25 08:35:24", "2014-09-25 09:04:00", "2014-09-25 09:04:09")
df = data.frame(ID,DateTime,speed, stringsAsFactors = FALSE)
df$DateTime<-as.POSIXct(df$DateTime, tz = "UTC")
library(dplyr)
df %>%
group_by(ID) %>%
mutate(timeDiff = c(NA, difftime(tail(DateTime, -1), head(DateTime, -1), units="sec"))) %>%
mutate(newID = paste(ID, cumsum(!is.na(timeDiff) & timeDiff > 100),sep = "_")) %>%
ungroup()
# prep the data
ID = c(rep("A",5), rep("B",5))
DateTime = c("2014-09-25 08:39:45", "2014-09-25 08:39:48", "2014-09-25 08:40:44", "2014-09-25 09:04:00","2014-09-25 09:04:10", "2014-09-25 08:33:32", "2014-09-25 08:34:41", "2014-09-25 08:35:24", "2014-09-25 09:04:00", "2014-09-25 09:04:09")
df = data.frame(ID,DateTime, stringsAsFactors = FALSE)
df$DateTime<-as.POSIXct(df$DateTime, tz = "UTC")
library(dplyr)
df %>%
group_by(ID) %>%
mutate(timeDiff = c(NA, difftime(tail(DateTime, -1), head(DateTime, -1), units="sec"))) %>%
mutate(newID = paste(ID, cumsum(!is.na(timeDiff) & timeDiff > 100),sep = "_")) %>%
ungroup()
