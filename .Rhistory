opts_chunk$set(fig.width=12,fig.height=4.5, error=TRUE,cache = F)
#' Record time for running all code
ptm<-proc.time()
#' Set the seed for the random number generator, so it will be possible
#' to reproduce the random points
set.seed(10299)
#' Create a login object for a user account at movebank.org
loginStored <- movebankLogin(username="MovebankWorkshop", password="genericuserforthisexercise")
getMovebankStudy(study="Martes pennanti LaPoint New York", login=loginStored) # see study-level info
#' Load data from a study in Movebank and create a MoveStack object. For more details and options see https://cran.r-project.org/web/packages/move/index.html.
fisher.move <- getMovebankData(study="Martes pennanti LaPoint New York", login=loginStored)
head(fisher.move)
#' Create a data frame from the MoveStack object
fisher.dat <- as(fisher.move, "data.frame")
#' Delete observations where missing lat or long or a timestamp.  There are no missing
#' observations in this data set, but it is still good practice to check.
ind<-complete.cases(fisher.dat[,c("location_lat", "location_long", "timestamp")])
fisher.dat<-fisher.dat[ind==TRUE,]
#' Check for duplicated observations (ones with same lat, long, timestamp,
#'  and individual identifier). There are no duplicate
#' observations in this data set, but it is still good practice to check.
ind2<-fisher.dat %>% dplyr::select(timestamp, location_long, location_lat, local_identifier) %>%
duplicated
sum(ind2) # no duplicates
fisher.dat<-fisher.dat[ind2!=TRUE,]
#' Make timestamp a date/time variable
fisher.dat$timestamp<-as.POSIXct(fisher.dat$timestamp, format="%Y-%m-%d %H:%M:%OS", tz="UTC")
#' Look at functions in the move package.
plot(fisher.move)
show(fisher.move)
summary(fisher.move)
fisherF2<-fisher.dat %>% filter(local_identifier=="F2")
z<-(calc_zoom(location_long, location_lat, fisherF2))
map <- get_map(location = c(lon = mean(fisherF2$location_long),
lat = mean(fisherF2$location_lat)), zoom = 12,
maptype = "hybrid", source = "google")
ggmap(map) +
geom_point(data=fisherF2, aes(x=location_long, y=location_lat), size=2.5)
#' Now, using leaflet
leaflet(fisherF2)%>%addTiles()%>%
addCircles(fisherF2$location_long, fisherF2$location_lat)
#' ### Using ggplot without a background
#'
#' Use separate axes for each individual (add scales="free" to facet_wrap)
#+fig.height=12, fig.width=12
ggplot(fisher.dat, aes(x=location_long, y=location_lat))+geom_point()+
facet_wrap(~local_identifier, scales="free")
#' Now, all on 1 plot
#+fig.height=6, fig.width=12
ggplot(fisher.dat, aes(x=location_long, y=location_lat, color=as.factor(local_identifier)))+
geom_point()
#' We can also use lat, long, which will allow us to determine
#' time of day
trk <- mk_track(fisher.dat, .x=location_long, .y=location_lat, .t=timestamp, id = local_identifier,
crs = CRS("+init=epsg:4326"))
# Now it is easy to calculate day/night with either movement track
trk <- trk %>% time_of_day()
#' Now, we can transform back to geographic coordinates
trk <- transform_coords(trk, CRS("+init=epsg:32618"))
#' Save the class here (and apply it later after adding columns to the
#' object)
trk.class<-class(trk)
#' individual (to avoid calculating a distance between say the last observation
#' of the first individual and the first observation of the second individual).
#'
#'
#' To do this, we could loop through individuals, calculate these
#' characteristics for each individual, then rbind the data
#' back together.  Or, use nested data frames and the map function
#' in the purrr library to do this with very little code.
#'
#' To see how nesting works, we can create a nested object by individual
nesttrk<-trk%>%nest(-id)
nesttrk
#' Each row contains data from an individual.  For example, we can access data
#' from the first individual using:
nesttrk$data[[1]]
#' We could calculate movement characteristics by individual using:
temp<-direction_rel(nesttrk$data[[1]])
head(temp)
#' or:
temp<-trk %>% filter(id=="M1") %>% direction_rel
head(temp)
#' Or, we can add a columns to each nested column of data using purrr::map
trk<-trk %>% nest(-id) %>%
mutate(dir_abs = map(data, direction_abs,full_circle=TRUE, zero="N"),
dir_rel = map(data, direction_rel),
sl = map(data, step_lengths),
nsd_=map(data, nsd))%>%unnest()
#' Now, calculate month, year, hour, week of each observation and append these to the dataset
#' Unlike the movement charactersitics, these calculations can be done all at once,
#' since they do not utilize successive observations (like step lengths and turn angles do).
trk<-trk%>%
mutate(
week=week(t_),
month = month(t_, label=TRUE),
year=year(t_),
hour = hour(t_)
)
#' Now, we need to again tell R that this is a track (rather
#' than just a data frame)
class(trk)
class(trk)<-trk.class
#' Lets take a look at what we created
trk
#' Notes:
#'
#' - It is common to generate points randomly, but other options are possible.
#' - In particular, it can beneficial to generate a systematically placed sample
#' - Samples can also be generated using the *spsample* function in the sp library or
#' using a GIS (note: amt uses the spsample function within its function random_points)
#' - Other home range polygons could be used (e.g., kernel density, local convex hull
#' etc.)
#'
#' #### Random points: illustrate for 1 individual
trk %>% filter(id=="F1") %>%
random_points(.,factor = 20) %>% plot
#' Illustrate systematic points (to do this, we need to create the mcp first)
trk%>%filter(id=="F1") %>%
random_points(., factor = 20, type="regular") %>%
plot()
#' Now, lets generate points for all individuals. We can do this
#' efficiently by making use of pipes (%>%),nested data frames, and
#' then by adding a new column -- a list-column -- to trks
avail.pts <- trk %>% nest(-id) %>%
mutate(rnd_pts = map(data, ~ random_points(., factor = 20, type="regular"))) %>%
select(id, rnd_pts) %>%  # you dont want to have the original point twice, hence drop data
unnest()
#' Now, lets generate points for all individuals. We can do this
#' efficiently by making use of pipes (%>%),nested data frames, and
#' then by adding a new column -- a list-column -- to trks
avail.pts <- trk %>% nest(-id) %>%
mutate(rnd_pts = map(data, ~ random_points(., factor = 20, type="regular"))) %>%
dplyr::select(id, rnd_pts) %>%  # you dont want to have the original point twice, hence drop data
unnest()
#'
#' Need to rename variables so everything is in the format Movebank requires for annotation of generic time-location
#' records (see https://www.movebank.org/node/6608#envdata_generic_request). This means, we need the following variables:
#'
#' - location-lat (perhaps with addition of Easting/Northing in UTMs)
#' - location-long (perhaps with addition of Easting/Northing in UTMs)
#' - timestamp (in Movebank format)
#'
#' Need to project to lat/long, while also keeping lat/long. Then rename
#' variables and write out the data sets.
avail <- SpatialPointsDataFrame(avail.pts[,c("x_","y_")], avail.pts,
proj4string=CRS("+proj=utm +zone=18N +datum=WGS84"))
avail.df <- data.frame(spTransform(avail, CRS("+proj=longlat +datum=WGS84")))[,1:6]
names(avail.df)<-c("idr", "case_", "utm.easting", "utm.northing", "location-long", "location-lat")
#' Check to make sure everything looks right
test<-subset(avail.df, case_==TRUE)
test %>% select('location-lat', 'location-long', utm.easting, utm.northing) %>%
summarise_all(mean)
fisher.dat %>% summarize(meanloc.lat=mean(location_lat),
meanloc.long=mean(location_long))
#' Check to make sure everything looks right
test<-subset(avail.df, case_==TRUE)
test %>% dplyr::select('location-lat', 'location-long', utm.easting, utm.northing) %>%
summarise_all(mean)
fisher.dat %>% summarize(meanloc.lat=mean(location_lat),
meanloc.long=mean(location_long))
#' Add a timestamp to annotate these data with environmental covariates in Movebank using Env-DATA (https://www.movebank.org/node/6607).
#' Here we just use the first timestamp, however meaningful timestamps are needed if annotating variables that vary in time.
avail.df$timestamp<-fisher.dat$timestamp[1]
#' ## SSF prep
#'
#' SSFs assume that data have been collected at regular time intervals.
#' We can use the track_resample function to regularize the trajectory so that
#' all points are located within some tolerence of each other in time. To figure
#' out a meaningful tolerance range, we should calculate time differences between
#' locations & look at as a function of individual.
(timestats<-trk %>% nest(-id) %>% mutate(sr = map(data, summarize_sampling_rate)) %>%
select(id, sr) %>% unnest)
#' ## SSF prep
#'
#' SSFs assume that data have been collected at regular time intervals.
#' We can use the track_resample function to regularize the trajectory so that
#' all points are located within some tolerence of each other in time. To figure
#' out a meaningful tolerance range, we should calculate time differences between
#' locations & look at as a function of individual.
(timestats<-trk %>% nest(-id) %>% mutate(sr = map(data, summarize_sampling_rate)) %>%
dplyr:: select(id, sr) %>% unnest)
#' Time intervals range from every 2 to 15 minutes on average, depending
#' on the individual.  Lets add on the time difference to each obs.
trk<-trk %>% group_by(id) %>% mutate(dt_ = t_ - lag(t_, default = NA))
#' Let's illustrate track regularization with ID = F2. Let's
#' go from every 2 minutes to every 10.
tempF2<-trk %>% filter(id=="F2") %>% track_resample(rate=minutes(10), tolerance=minutes(2))
tempF2 %>% select(id, x_, y_, t_, burst_)
#' Let's illustrate track regularization with ID = F2. Let's
#' go from every 2 minutes to every 10.
tempF2<-trk %>% filter(id=="F2") %>% track_resample(rate=minutes(10), tolerance=minutes(2))
tempF2 %>% dplyr::select(id, x_, y_, t_, burst_)
#+warning=FALSE
ssfdat<-NULL
temptrk<-with(trk, track(x=x_, y=y_, t=t_, id=id))
uid<-unique(trk$id) # individual identifiers
luid<-length(uid) # number of unique individuals
for(i in 1:luid){
# Subset individuals & regularize track
temp<-temptrk%>% filter(id==uid[i]) %>%
track_resample(rate=minutes(round(timestats$median[i])),
tolerance=minutes(max(10,round(timestats$median[i]/5))))
# Get rid of any bursts without at least 2 points
temp<-filter_min_n_burst(temp, 2)
# burst steps
stepstemp<-steps_by_burst(temp)
# create random steps using fitted gamma and von mises distributions and append
rnd_stps <- stepstemp %>%  random_steps(n = 15)
# append id
rnd_stps<-rnd_stps%>%mutate(id=uid[i])
# append new data to data from other individuals
ssfdat<-rbind(rnd_stps, ssfdat)
}
#' Load libraries
#+warning=FALSE, message=FALSE
library(knitr)
library(lubridate)
library(raster)
library(move)
library(amt)
library(sp)
# select ck_tanz data which is Corinne's data from everything
ck_tanz_data <- filter(mydata, study == "ck_tanz")
ck_tanz_data
#' Check for duplicated observations (ones with same lat, long, timestamp,
#'  and individual identifier).
ind2<-ck_tanz_data %>% select(long, lat, id) %>%
duplicated
library(tidyverse)
sum(ind2)
#' Check for duplicated observations (ones with same lat, long, timestamp,
#'  and individual identifier).
ind2<-ck_tanz_data %>% select(long, lat, id) %>%
duplicated
library(amt)
#########################################################################
#' Vulture comparative analysis
#' tutorials here https://www.jessesadler.com/post/gis-with-r-intro/
#' and here https://www.r-spatial.org/
#' 06 November 2018
#' 1_load_data - this loads in all of the tracking data and binds it
#########################################################################
#' Load the required packages
library(readr)
library(tidyverse)
#' Section 1: Load the data ----
data_path <- "raw_data"   # path to the data
files <- dir(data_path, pattern = "*.csv") # get file names
mydata <- files %>%
# read in all the files, appending the path before the filename
map(~ read_csv(file.path(data_path, .))) %>%
reduce(rbind)
mydata <- files %>%
# read in all the files, appending the path before the filename
map(~ read_csv(file.path(data_path, .))) %>%
reduce(rbind)
#########################################################################
#' Vulture comparative analysis
#' tutorials here https://www.jessesadler.com/post/gis-with-r-intro/
#' and here https://www.r-spatial.org/
#' 06 November 2018
#' 1_load_data - this loads in all of the tracking data and binds it
#########################################################################
#' Load the required packages
library(readr)
library(tidyverse)
#' Section 1: Load the data ----
data_path <- "raw_data"   # path to the data
files <- dir(data_path, pattern = "*.csv") # get file names
mydata <- files %>%
# read in all the files, appending the path before the filename
map(~ read_csv(file.path(data_path, .))) %>%
reduce(rbind)
#########################################################################
#' Vulture comparative analysis
#' tutorials here https://www.jessesadler.com/post/gis-with-r-intro/
#' and here https://www.r-spatial.org/
#' 06 November 2018
#' 1_load_data - this loads in all of the tracking data and binds it
#########################################################################
#' Load the required packages
library(readr)
library(tidyverse)
#' Section 1: Load the data ----
data_path <- "raw_data"   # path to the data
files <- dir(data_path, pattern = "*.csv") # get file names
mydata <- files %>%
# read in all the files, appending the path before the filename
map(~ read_csv(file.path(data_path, .))) %>%
reduce(rbind)
library(lubridate)
library(SDLfilter)
library(amt)
library(sp)
# select swazi data which is the data we tracked in Swaziland
swazi_data <- filter(mydata, study == "swazi")
swazi_data
#' Check for duplicated observations (ones with same lat, long, timestamp,
#'  and individual identifier).
ind2<-swazi_data %>% select(long, lat, id) %>%
duplicated
sum(ind2)
#' Check for duplicated observations (ones with same lat, long, timestamp,
#'  and individual identifier).
ind2<-swazi_data %>% select(timestamp, long, lat, id) %>%
duplicated
head(swazi_data)
#' Check for duplicated observations (ones with same lat, long, timestamp,
#'  and individual identifier).
ind2<-swazi_data %>% select(time, long, lat, id) %>%
duplicated
sum(ind2)
#########################################################################
#' Vulture comparative analysis
#' tutorials here https://www.jessesadler.com/post/gis-with-r-intro/
#' and here https://www.r-spatial.org/
#' 06 November 2018
#' 1_load_data - this loads in all of the tracking data and binds it
#########################################################################
#' Load the required packages
library(readr)
library(tidyverse)
#' Section 1: Load the data ----
data_path <- "raw_data"   # path to the data
files <- dir(data_path, pattern = "*.csv") # get file names
mydata <- files %>%
# read in all the files, appending the path before the filename
map(~ read_csv(file.path(data_path, .))) %>%
reduce(rbind)
#' filter the data to remove obvious outliers
mydata <- filter(mydata, lat < 20 & lat > -40 & long > 10)
head(mydata)
tail(mydata)
str(mydata)
levels(as.factor(mydata$study))
library(lubridate)
library(SDLfilter)
library(amt)
library(sp)
# select swazi data which is the data we tracked in Swaziland
swazi_data <- filter(mydata, study == "swazi")
swazi_data
#' Check for duplicated observations (ones with same lat, long, timestamp,
#'  and individual identifier).
ind2<-swazi_data %>% select(time, long, lat, id) %>%
duplicated
sum(ind2)
# remove them
swazi_data$dups <- ind2
swazi_data <- filter(swazi_data,dups=="FALSE")
swazi_data
# set the time column
levels(factor(swazi_data$id))
# can look at an individual level with
(filter(swazi_data,id=="ID1"))
# all of the data is in the format of day-month-year
swazi_data$New_time<-parse_date_time(x=swazi_data$time,c("%d/%m/%Y %H:%M"))
# keep only the new time data
swazi_data <- select(swazi_data, New_time,long,lat,id,species,study)
swazi_data <- rename(swazi_data, time = New_time)
swazi_data
# check the minimum time and the maximum time
min_time <- swazi_data %>% group_by(id) %>% slice(which.min(time))
data.frame(min_time)
max_time <- swazi_data %>% group_by(id) %>% slice(which.max(time))
data.frame(max_time)
#' filter extreme data based on a speed threshold
#' based on vmax which is km/hr
#' time needs to be labelled DateTime for these functions to work
names(swazi_data)[names(swazi_data) == 'time'] <- 'DateTime'
SDLfilterData<-ddfilter.speed(data.frame(swazi_data), vmax = 60, method = 1)
(40038/301986)*100
length(SDLfilterData$DateTime)
#' rename everything as before
swazi_data <- SDLfilterData
names(swazi_data)[names(swazi_data) == 'DateTime'] <- 'time'
# try the amt package
trk <- mk_track(swazi_data, .x=long, .y=lat, .t=time, id = id, species=species,
crs = CRS("+init=epsg:4326"))
# Now it is easy to calculate day/night with either movement track
trk <- trk %>% time_of_day()
#' Now, we can transform back to geographic coordinates
trk <- transform_coords(trk, CRS("+init=epsg:32618"))
trk.class<-class(trk)
nesttrk<-trk%>%nest(-id)
nesttrk
trk<-trk %>% nest(-id) %>%
mutate(dir_abs = map(data, direction_abs,full_circle=TRUE, zero="N"),
dir_rel = map(data, direction_rel),
sl = map(data, step_lengths),
nsd_=map(data, nsd))%>%unnest()
trk<-trk%>%
mutate(
week=week(t_),
month = month(t_, label=TRUE),
year=year(t_),
hour = hour(t_)
)
class(trk)
class(trk)<-trk.class
#' Lets take a look at what we created
trk
#' look at net-squared displacement
ggplot(trk, aes(x = t_, y=nsd_)) + geom_point()+
facet_wrap(~id, scales="free")
#' look at net-squared displacement
ggplot(trk, aes(x = t_, y = nsd_)) + geom_point() +
facet_wrap( ~ id, scales = "free")
#' export the plot
swazi_net_disp <- ggplot(trk, aes(x = t_, y=nsd_)) + geom_point()+
facet_wrap(~id, scales="free")
ggsave("plots/swazi_net_disp.pdf")
ggsave("plots/swazi_net_disp.png")
#' ### Absolute angles (for each movement) relative to North
#' We could use a rose diagram (below) to depict the distribution of angles.
#+fig.height=12, fig.width=12
ggplot(trk, aes(x = dir_abs, y = ..density..)) + geom_histogram(breaks = seq(0, 360, by =
20)) +
coord_polar(start = 0) + theme_minimal() +
scale_fill_brewer() + ylab("Density") + ggtitle("Angles Direct") +
scale_x_continuous(
"",
limits = c(0, 360),
breaks = seq(0, 360, by = 20),
labels = seq(0, 360, by = 20)
) +
facet_wrap( ~ id)
#' ### Turning angles
#'
#' Note: a 0 indicates the animal continued to move in a straight line, a 180
#' indicates the animal turned around (but note, resting + measurement error often can
#' make it look like the animal turned around).
#+fig.height=12, fig.width=12
ggplot(trk, aes(x = dir_rel, y = ..density..)) + geom_histogram(breaks = seq(-180, 180, by =
20)) +
coord_polar(start = 0) + theme_minimal() +
scale_fill_brewer() + ylab("Density") + ggtitle("Angles Direct") +
scale_x_continuous(
"",
limits = c(-180, 180),
breaks = seq(-180, 180, by = 20),
labels = seq(-180, 180, by = 20)
) +
facet_wrap(~ id)
#' Home range data
mcps.week <- trk %>%
mutate(year = year(t_),
month = month(t_),
week = week(t_)) %>%
group_by(id, year, month, week) %>% nest() %>%
mutate(mcparea = map(data, ~ hr_mcp(., levels = c(0.95)) %>% hr_area)) %>%  select(id, year, month, week, mcparea) %>% unnest()
#' home range plots
ggplot(mcps.week, aes(x = week, y = area, colour = factor(year))) +
geom_point() +
geom_smooth() + facet_wrap(~id, scales="free")
#' Kernel utilisation distributions
kde.week <- trk %>%
mutate(year = year(t_), month = month(t_), week = week(t_)) %>%
group_by(id, year, month, week) %>% nest() %>%
mutate(kdearea = map(data, ~ hr_kde(., levels=c(0.95)) %>% hr_area)) %>%
select(id, year, month, week, kdearea) %>% unnest()
ggplot(kde.week, aes(x = week, y = kdearea, colour = factor(year))) +
geom_point() +
geom_smooth() + facet_wrap(~id, scales = "free")
kde.week
#' plot the tracks
ggplot(swazi_data, aes(x = location_long,
y = location_lat)) + geom_point() +
facet_wrap(~local_identifier, scales = "free")
head(swazi_data)
#' plot the tracks
ggplot(swazi_data, aes(x = long,
y = lat)) + geom_point() +
facet_wrap(~id, scales = "free")
library(lubridate)
library(SDLfilter)
library(amt)
library(sp)
library(ggplot2)
library(moveVis)
library(move)
library(magrittr)
library(raster)
#' select swazi data which is the data we tracked in Swaziland
swazi_data <- filter(mydata, study == "swazi")
swazi_data
#########################################################################
#' Vulture comparative analysis
#' tutorials here https://www.jessesadler.com/post/gis-with-r-intro/
#' and here https://www.r-spatial.org/
#' 06 November 2018
#' 1_load_data - this loads in all of the tracking data and binds it
#########################################################################
#' Load the required packages
library(readr)
library(tidyverse)
#' Section 1: Load the data ----
data_path <- "raw_data"   # path to the data
files <- dir(data_path, pattern = "*.csv") # get file names
mydata <- files %>%
# read in all the files, appending the path before the filename
map(~ read_csv(file.path(data_path, .))) %>%
reduce(rbind)
#' filter the data to remove obvious outliers
mydata <- filter(mydata, lat < 20 & lat > -40 & long > 10)
head(mydata)
tail(mydata)
str(mydata)
levels(as.factor(mydata$study))
library(lubridate)
library(SDLfilter)
library(amt)
library(sp)
library(ggplot2)
library(moveVis)
library(move)
library(magrittr)
library(raster)
#' select swazi data which is the data we tracked in Swaziland
swazi_data <- filter(mydata, study == "swazi")
swazi_data
#' Check for duplicated observations (ones with same lat, long, timestamp,
#'  and individual identifier).
ind2 <- swazi_data %>% dplyr::select(time, long, lat, id) %>%
duplicated
sum(ind2)
# remove them
swazi_data$dups <- ind2
swazi_data <- filter(swazi_data, dups == "FALSE")
swazi_data
# set the time column
levels(factor(swazi_data$id))
# can look at an individual level with
(filter(swazi_data, id == "ID1"))
# all of the data is in the format of day-month-year
swazi_data$New_time <-
parse_date_time(x = swazi_data$time, c("%d/%m/%Y %H:%M"))
# keep only the new time data
swazi_data <-
dplyr::select(swazi_data, New_time, long, lat, id, species, study)
swazi_data <- rename(swazi_data, time = New_time)
swazi_data
# check the minimum time and the maximum time
min_time <- swazi_data %>% group_by(id) %>% slice(which.min(time))
data.frame(min_time)
max_time <- swazi_data %>% group_by(id) %>% slice(which.max(time))
data.frame(max_time)
#' filter extreme data based on a speed threshold
#' based on vmax which is km/hr
#' time needs to be labelled DateTime for these functions to work
names(swazi_data)[names(swazi_data) == 'time'] <- 'DateTime'
SDLfilterData <-
ddfilter.speed(data.frame(swazi_data), vmax = 60, method = 1)
length(SDLfilterData$DateTime)
#' rename everything as before
swazi_data <- SDLfilterData
names(swazi_data)[names(swazi_data) == 'DateTime'] <- 'time'
# try the amt package
trk <- mk_track(swazi_data, .x=long, .y=lat, .t=time, id = id, species=species,
crs = CRS("+init=epsg:4326"))
# Now it is easy to calculate day/night with either movement track
trk <- trk %>% time_of_day()
head(trk)
#' subset a sample of the data using head for an example
sample_data <- dplyr::filter(trk, id == "ID1" & tod_ == "day")
#' transform the data into a move object
sample_data <-
move(
x = sample_data$x_,
y = sample_data$y_,
time = sample_data$t_,
proj = CRS("+proj=longlat +ellps=WGS84")
)
#'
#' ### Make frames of tracking data for animation.
#' Evaluate tracking data for sampling rates if unknown. Use this information to help decide
#' the temporal resolution at which to align the data for the animation.
unique(timestamps(sample_data))
timeLag(sample_data, unit = "mins")
#' pick specific rows
sample_data <- slice(data.frame(sample_data), 235:435)
#' turn it back into a move object
sample_data <-
move(
x = sample_data$x,
y = sample_data$y,
time = sample_data$time,
proj = CRS("+proj=longlat +ellps=WGS84")
)
#' Align tracking data to uniform temporal resolution for interpretation by frames_spatial.
vultures <-
align_move(sample_data,
res = 1,
unit = "mins",
spaceMethod = "greatcircle")
frames <-
frames_spatial(
vultures,
map_service = "osm",
map_type = "watercolor",
equidistant = FALSE,
path_legend = T,
path_legend_title = "Vulture",
alpha = 0.5
)
#' Add labels and a progress bar.
frames.l <-
add_labels(frames, x = "Longitude", y = "Latitude") %>% # axis labels
add_progress() %>% # progress bar
add_scalebar() %>% # scale bar
add_northarrow() %>% # north arrow
add_timestamps(geese, type = "label") # timestamps
#' Add labels and a progress bar.
frames.l <-
add_labels(frames, x = "Longitude", y = "Latitude") %>% # axis labels
add_progress() %>% # progress bar
add_scalebar() %>% # scale bar
add_northarrow() %>% # north arrow
add_timestamps(vultures, type = "label") # timestamps
#' Record an animation using defaults shown in in moveVis manual p.18.
animate_frames(
frames.l,
"animations/swazi_vulture_test.gif",
fps = 25,
width = 500,
height = 800,
res = 100,
display = TRUE,
overwrite = TRUE,
verbose = TRUE
)
library(lubridate)
library(raster)
library(move)
library(amt)
library(sp)
library(moveVis)
library(move)
library(ggplot2)
library(raster)
library(magrittr)
#########################################################################
#' Vulture comparative analysis
#' tutorials here https://www.jessesadler.com/post/gis-with-r-intro/
#' and here https://www.r-spatial.org/
#' 06 November 2018
#' 1_load_data - this loads in all of the tracking data and binds it
#########################################################################
#' Load the required packages
library(readr)
library(tidyverse)
#' Section 1: Load the data ----
data_path <- "raw_data"   # path to the data
files <- dir(data_path, pattern = "*.csv") # get file names
mydata <- files %>%
# read in all the files, appending the path before the filename
map(~ read_csv(file.path(data_path, .))) %>%
reduce(rbind)
#' filter the data to remove obvious outliers
mydata <- filter(mydata, lat < 20 & lat > -40 & long > 10)
head(mydata)
tail(mydata)
str(mydata)
levels(as.factor(mydata$study))
length(levels(as.factor(mydata$study)))
glimpse(mydata)
wb <- filter(mydata, species == "wb")
length(levels(wb$id))
(levels(wb$id))
(class(wb$id))
wb$id <- as.factor(wb$id)
length(levels(wb$id))
levels(as.factor(mydata$study))
head(mydata)
