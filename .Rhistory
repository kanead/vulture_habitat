select(id, kdearea) %>% unnest()
kde$kdearea <-  kde$kdearea / 1000000
kde_95 <- kde %>% arrange(id)
#' combine the summary stats
data_summary$duration <- duration
data_summary$min_time <- min_time$time
data_summary$max_time <- max_time$time
data_summary$kde <- kde_95$kdearea
data_summary$mcps <- mcp_95$area
data_summary$species <- min_time$species
data_summary
#' We can map the data
#' turn back to lat long
trk_map <-
mk_track(
mend_data,
.x = long,
.y = lat,
.t = time,
id = id,
species = species,
crs = CRS("+init=epsg:4326")
)
#' plot all of the data on the one graph
library(ggmap)
qmplot(x_,
y_,
data = trk_map,
maptype = "toner-lite",
colour = id)
(
qmplot(
x_,
y_,
data = trk_map,
maptype = "toner-background",
colour = id
) +
facet_wrap( ~ id)
)
# WBV Namibia Vulture Tracking Dataset
library(lubridate)
library(SDLfilter)
library(amt)
library(sp)
# ga_nam
# select ga_nam data which is data from everything
ga_nam <- filter(mydata, study == "ga_nam")
ga_nam
#' Check for duplicated observations (ones with same lat, long, timestamp,
#'  and individual identifier).
ind2<-ga_nam %>% select(long, lat, id) %>%
duplicated
sum(ind2)
# remove them
ga_nam$dups <- ind2
ga_nam <- filter(ga_nam,dups=="FALSE")
ga_nam
# set the time column
levels(factor(ga_nam$id))
# can look at an individual level with
(filter(ga_nam,id=="5864"))
# all of the data is in the format of day-month-year
ga_nam$New_time<-parse_date_time(x=ga_nam$time,c("%d/%m/%Y %H:%M"))
#' all of the data is in the format of day-month-year
#' time zone is UTC by default
ga_nam$New_time <-
parse_date_time(x = ga_nam$time, c("%d/%m/%Y %H:%M"))
#' keep only the new time data
ga_nam <- select(ga_nam, New_time,long,lat,id,species,study)
ga_nam <- rename(ga_nam, time = New_time)
ga_nam
#' filter extreme data based on a speed threshold
#' based on vmax which is km/hr
#' time needs to be labelled DateTime for these functions to work
names(ga_nam)[names(ga_nam) == 'time'] <- 'DateTime'
SDLfilterData <-
ddfilter.speed(data.frame(ga_nam), vmax = 70, method = 1)
length(SDLfilterData$DateTime)
#' rename everything as before
ga_nam <- SDLfilterData
names(ga_nam)[names(ga_nam) == 'DateTime'] <- 'time'
# check the minimum time and the maximum time
min_time <- ga_nam %>% group_by(id) %>% slice(which.min(time))
data.frame(min_time)
max_time <- ga_nam %>% group_by(id) %>% slice(which.max(time))
data.frame(max_time)
#' one of the IDs (5863) has a date in 2025!
#' This seems to be only the last few rows
#' We can delete anything that comes after a certain date
ga_nam <- ga_nam %>% filter(time < "2019-01-01")
#' check the dates again
max_time <- ga_nam %>% group_by(id) %>% slice(which.max(time))
data.frame(max_time)
#' determine the length of time each bird was tracked for
duration <- difftime(max_time$time, min_time$time, units = "days"); duration
# try the amt package
trk <-
mk_track(
ga_nam,
.x = long,
.y = lat,
.t = time,
id = id,
species = species,
crs = CRS("+init=epsg:4326")
) %>%
transform_coords(
sp::CRS(
#' we can transform the CRS of the data to an equal area projection
"+proj=aea +lat_1=20 +lat_2=-23 +lat_0=0 +lon_0=25 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs"
)
)
#' summarise the sampling rate
data_summary <- trk %>% nest(-id) %>% mutate(sr = map(data, summarize_sampling_rate)) %>%
dplyr::select(id, sr) %>% unnest %>% arrange(id) ; sampling_rate
#' Calculate home range size for data that is not regularised
mcps <- trk %>% nest(-id) %>%
mutate(mcparea = map(data, ~ hr_mcp(., levels = c(0.95)) %>% hr_area)) %>%
select(id, mcparea) %>% unnest()
mcps$area <- mcps$area / 1000000
mcp_95 <- mcps %>% arrange(id)
#' Same for KDE
kde <- trk %>% nest(-id) %>%
mutate(kdearea = map(data, ~ hr_kde(., levels = c(0.95)) %>% hr_area)) %>%
select(id, kdearea) %>% unnest()
kde$kdearea <-  kde$kdearea / 1000000
kde_95 <- kde %>% arrange(id)
#' combine the summary stats
data_summary$duration <- duration
data_summary$min_time <- min_time$time
data_summary$max_time <- max_time$time
data_summary$kde <- kde_95$kdearea
data_summary$mcps <- mcp_95$area
data_summary$species <- min_time$species
data_summary
#' plot all of the data on the one graph
library(ggmap)
qmplot(x_,
y_,
data = trk_map,
maptype = "toner-lite",
colour = id)
#' plot each track on a separate panel using facet
(
qmplot(
x_,
y_,
data = trk_map,
maptype = "toner-background",
colour = id
) +
facet_wrap( ~ id)
)
#' We can map the data
#' turn back to lat long
trk_map <-
mk_track(
ga_nam,
.x = long,
.y = lat,
.t = time,
id = id,
species = species,
crs = CRS("+init=epsg:4326")
)
#' plot all of the data on the one graph
library(ggmap)
qmplot(x_,
y_,
data = trk_map,
maptype = "toner-lite",
colour = id)
#' plot all of the data on the one graph
library(ggmap)
#' plot each track on a separate panel using facet
(
qmplot(
x_,
y_,
data = trk_map,
maptype = "toner-background",
colour = id
) +
facet_wrap( ~ id)
)
# WBV Namibia Vulture Tracking Dataset
library(lubridate)
library(SDLfilter)
library(amt)
library(sp)
# ga_nam
# select ga_nam data which is data from everything
ga_nam <- filter(mydata, study == "ga_nam")
ga_nam
#' Check for duplicated observations (ones with same lat, long, timestamp,
#'  and individual identifier).
ind2<-ga_nam %>% select(long, lat, id) %>%
duplicated
sum(ind2)
# remove them
ga_nam$dups <- ind2
ga_nam <- filter(ga_nam,dups=="FALSE")
ga_nam
# set the time column
levels(factor(ga_nam$id))
# can look at an individual level with
(filter(ga_nam,id=="5864"))
# all of the data is in the format of day-month-year
ga_nam$New_time<-parse_date_time(x=ga_nam$time,c("%d/%m/%Y %H:%M"))
#' all of the data is in the format of day-month-year
#' time zone is UTC by default
ga_nam$New_time <-
parse_date_time(x = ga_nam$time, c("%d/%m/%Y %H:%M"))
#' keep only the new time data
ga_nam <- select(ga_nam, New_time,long,lat,id,species,study)
ga_nam <- rename(ga_nam, time = New_time)
ga_nam
#' filter extreme data based on a speed threshold
#' based on vmax which is km/hr
#' time needs to be labelled DateTime for these functions to work
names(ga_nam)[names(ga_nam) == 'time'] <- 'DateTime'
SDLfilterData <-
ddfilter.speed(data.frame(ga_nam), vmax = 80, method = 1)
length(SDLfilterData$DateTime)
#' rename everything as before
ga_nam <- SDLfilterData
names(ga_nam)[names(ga_nam) == 'DateTime'] <- 'time'
# check the minimum time and the maximum time
min_time <- ga_nam %>% group_by(id) %>% slice(which.min(time))
data.frame(min_time)
max_time <- ga_nam %>% group_by(id) %>% slice(which.max(time))
data.frame(max_time)
#' one of the IDs (5863) has a date in 2025!
#' This seems to be only the last few rows
#' We can delete anything that comes after a certain date
ga_nam <- ga_nam %>% filter(time < "2019-01-01")
#' check the dates again
max_time <- ga_nam %>% group_by(id) %>% slice(which.max(time))
data.frame(max_time)
#' determine the length of time each bird was tracked for
duration <- difftime(max_time$time, min_time$time, units = "days"); duration
# try the amt package
trk <-
mk_track(
ga_nam,
.x = long,
.y = lat,
.t = time,
id = id,
species = species,
crs = CRS("+init=epsg:4326")
) %>%
transform_coords(
sp::CRS(
#' we can transform the CRS of the data to an equal area projection
"+proj=aea +lat_1=20 +lat_2=-23 +lat_0=0 +lon_0=25 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs"
)
)
#' summarise the sampling rate
data_summary <- trk %>% nest(-id) %>% mutate(sr = map(data, summarize_sampling_rate)) %>%
dplyr::select(id, sr) %>% unnest %>% arrange(id) ; sampling_rate
#' Calculate home range size for data that is not regularised
mcps <- trk %>% nest(-id) %>%
mutate(mcparea = map(data, ~ hr_mcp(., levels = c(0.95)) %>% hr_area)) %>%
select(id, mcparea) %>% unnest()
mcps$area <- mcps$area / 1000000
mcp_95 <- mcps %>% arrange(id)
#' Same for KDE
kde <- trk %>% nest(-id) %>%
mutate(kdearea = map(data, ~ hr_kde(., levels = c(0.95)) %>% hr_area)) %>%
select(id, kdearea) %>% unnest()
kde$kdearea <-  kde$kdearea / 1000000
kde_95 <- kde %>% arrange(id)
#' combine the summary stats
data_summary$duration <- duration
data_summary$min_time <- min_time$time
data_summary$max_time <- max_time$time
data_summary$kde <- kde_95$kdearea
data_summary$mcps <- mcp_95$area
data_summary$species <- min_time$species
data_summary
594048 - 254719
?ddfilter.speed
#' estimate vmax for threshold speed
est.vmax(sdata = swazi_data, qi = 5, prob = 0.99)
# Swaziland Vulture Tracking Dataset
library(lubridate)
library(SDLfilter)
library(amt)
library(sp)
# swazi
# select swazi data which is the data we tracked in Swaziland
swazi_data <- filter(mydata, study == "swazi")
swazi_data
#' Check for duplicated observations (ones with same lat, long, timestamp,
#'  and individual identifier).
ind2<-swazi_data %>% select(time, long, lat, id) %>%
duplicated
sum(ind2)
# remove them
swazi_data$dups <- ind2
swazi_data <- filter(swazi_data,dups=="FALSE")
swazi_data
# set the time column
levels(factor(swazi_data$id))
# can look at an individual level with
(filter(swazi_data,id=="ID1"))
# all of the data is in the format of day-month-year
swazi_data$New_time<-parse_date_time(x=swazi_data$time,c("%d/%m/%Y %H:%M"))
# keep only the new time data
swazi_data <- select(swazi_data, New_time,long,lat,id,species,study)
swazi_data <- rename(swazi_data, time = New_time)
swazi_data
#' estimate vmax for threshold speed
est.vmax(sdata = swazi_data, qi = 5, prob = 0.99)
#' filter extreme data based on a speed threshold
#' based on vmax which is km/hr
#' time needs to be labelled DateTime for these functions to work
names(swazi_data)[names(swazi_data) == 'time'] <- 'DateTime'
#' estimate vmax for threshold speed
est.vmax(sdata = swazi_data, qi = 5, prob = 0.99)
#' estimate vmax for threshold speed
speed.est.data <- swazi_data %>% select(id,DateTime,lat,long)
est.vmax(sdata = speed.est.data, qi = 5, prob = 0.99)
speed.est.data
est.vmax(sdata = data.frame(speed.est.data), qi = 5, prob = 0.99)
speed.est.data
speed.est.data$qi = 5
speed.est.data
?est.vmax
#' estimate vmax for threshold speed
speed.est.data <- swazi_data %>% select(id,DateTime,lat,long)
speed.est.data$qi = 5
est.vmax(sdata = data.frame(speed.est.data))
#' estimate vmax for threshold speed
speed.est.data <- swazi_data %>% filter(id == "ID1") %>%  select(id,DateTime,lat,long)
speed.est.data$qi = 5
est.vmax(sdata = data.frame(speed.est.data))
#' estimate vmax for threshold speed
speed.est.data <- swazi_data %>% filter(id == "ID2") %>%  select(id,DateTime,lat,long)
speed.est.data$qi = 5
est.vmax(sdata = data.frame(speed.est.data))
# Morgan Pfeiffer's SA Vulture Tracking Dataset
library(lubridate)
library(SDLfilter)
library(amt)
library(sp)
# select Morgan's data
morgan_data <- filter(mydata, study == "pfeiffer")
morgan_data
# drop missing rows
morgan_data<- morgan_data %>% drop_na
#' Check for duplicated observations (ones with same lat, long, timestamp,
#'  and individual identifier).
ind2<-morgan_data %>% select(long, lat, id) %>%
duplicated
sum(ind2)
# remove them
morgan_data$dups <- ind2
morgan_data <- filter(morgan_data,dups=="FALSE")
morgan_data
# set the time column
# some are in day/month/year format e.g. X016_Complete; X020_Final; X021_Final; X022_Complete; X032_Final; X033_Complete;
# some are in month/day/year format e.g. X023; X027; X042; X050; X051; X052; X053; X055; X056; X057; X071
levels(factor(morgan_data$id))
temp1<-filter(morgan_data,
id == "X016_Complete" |
id=="X021_Final" |
id == "X020_Final" |
id == "X021_Final" |
id == "X022_Complete" |
id == "X032_Final" |
id == "X033_Complete" ); tail(temp1) ;head(temp1)
temp1
temp1$New_time<-parse_date_time(x=temp1$time,c("%d/%m/%Y %H:%M"))
tail(temp1)
temp2<-filter(morgan_data,
id == "X023" |
id == "X027" |
id == "X042" |
id == "X050" |
id == "X051" |
id == "X052" |
id == "X053" |
id == "X055" |
id == "X056" |
id == "X057" |
id == "X071" ); tail(temp2) ;head(temp2)
temp2
temp2$New_time<-parse_date_time(x=temp2$time,c("%m/%d/%Y %H:%M"))
tail(temp2)
# stick them back together again
morgan_data <- full_join(temp1,temp2)
morgan_data
# Morgan's data is in reverse order of time
# sort by the bird ID and reverse the order
morgan_data <- morgan_data %>% group_by(id)  %>%
arrange(New_time, .by_group = TRUE)
morgan_data
# keep only the new time data
morgan_data <- select(morgan_data, New_time,long,lat,id,species,study)
morgan_data <- rename(morgan_data, time = New_time)
#' estimate vmax for threshold speed
#' names(morgan_data)[names(morgan_data) == 'time'] <- 'DateTime'
#' speed.est.data <- morgan_data %>% select(id,DateTime,lat,long)
#' speed.est.data$qi = 5
#' est.vmax(sdata = data.frame(speed.est.data))
names(morgan_data)[names(morgan_data) == 'time'] <- 'DateTime'
speed.est.data <- morgan_data %>% select(id,DateTime,lat,long)
speed.est.data$qi = 5
est.vmax(sdata = data.frame(speed.est.data))
# WBV Namibia Vulture Tracking Dataset
library(lubridate)
library(SDLfilter)
library(amt)
library(sp)
# ga_nam
# select ga_nam data which is data from everything
ga_nam <- filter(mydata, study == "ga_nam")
ga_nam
#' Check for duplicated observations (ones with same lat, long, timestamp,
#'  and individual identifier).
ind2<-ga_nam %>% select(long, lat, id) %>%
duplicated
sum(ind2)
# remove them
ga_nam$dups <- ind2
ga_nam <- filter(ga_nam,dups=="FALSE")
ga_nam
# set the time column
levels(factor(ga_nam$id))
# can look at an individual level with
(filter(ga_nam,id=="5864"))
# all of the data is in the format of day-month-year
ga_nam$New_time<-parse_date_time(x=ga_nam$time,c("%d/%m/%Y %H:%M"))
#' all of the data is in the format of day-month-year
#' time zone is UTC by default
ga_nam$New_time <-
parse_date_time(x = ga_nam$time, c("%d/%m/%Y %H:%M"))
#' keep only the new time data
ga_nam <- select(ga_nam, New_time,long,lat,id,species,study)
ga_nam <- rename(ga_nam, time = New_time)
ga_nam
names(ga_nam)[names(ga_nam) == 'time'] <- 'DateTime'
speed.est.data <- ga_nam %>% select(id,DateTime,lat,long)
speed.est.data$qi = 5
est.vmax(sdata = data.frame(speed.est.data))
#' filter extreme data based on a speed threshold
#' based on vmax which is km/hr
#' time needs to be labelled DateTime for these functions to work
names(ga_nam)[names(ga_nam) == 'time'] <- 'DateTime'
SDLfilterData <-
ddfilter.speed(data.frame(ga_nam), vmax = 70, method = 1)
SDLfilterData
#' rename everything as before
ga_nam <- SDLfilterData
names(ga_nam)[names(ga_nam) == 'DateTime'] <- 'time'
# check the minimum time and the maximum time
min_time <- ga_nam %>% group_by(id) %>% slice(which.min(time))
data.frame(min_time)
max_time <- ga_nam %>% group_by(id) %>% slice(which.max(time))
data.frame(max_time)
#' one of the IDs (5863) has a date in 2025!
#' This seems to be only the last few rows
#' We can delete anything that comes after a certain date
ga_nam <- ga_nam %>% filter(time < "2019-01-01")
#' check the dates again
max_time <- ga_nam %>% group_by(id) %>% slice(which.max(time))
data.frame(max_time)
#' determine the length of time each bird was tracked for
duration <- difftime(max_time$time, min_time$time, units = "days"); duration
# try the amt package
trk <-
mk_track(
ga_nam,
.x = long,
.y = lat,
.t = time,
id = id,
species = species,
crs = CRS("+init=epsg:4326")
) %>%
transform_coords(
sp::CRS(
#' we can transform the CRS of the data to an equal area projection
"+proj=aea +lat_1=20 +lat_2=-23 +lat_0=0 +lon_0=25 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs"
)
)
#' summarise the sampling rate
data_summary <- trk %>% nest(-id) %>% mutate(sr = map(data, summarize_sampling_rate)) %>%
dplyr::select(id, sr) %>% unnest %>% arrange(id) ; sampling_rate
#' Calculate home range size for data that is not regularised
mcps <- trk %>% nest(-id) %>%
mutate(mcparea = map(data, ~ hr_mcp(., levels = c(0.95)) %>% hr_area)) %>%
select(id, mcparea) %>% unnest()
mcps$area <- mcps$area / 1000000
mcp_95 <- mcps %>% arrange(id)
#' Same for KDE
kde <- trk %>% nest(-id) %>%
mutate(kdearea = map(data, ~ hr_kde(., levels = c(0.95)) %>% hr_area)) %>%
select(id, kdearea) %>% unnest()
kde$kdearea <-  kde$kdearea / 1000000
kde_95 <- kde %>% arrange(id)
#' combine the summary stats
data_summary$duration <- duration
data_summary$min_time <- min_time$time
data_summary$max_time <- max_time$time
data_summary$kde <- kde_95$kdearea
data_summary$mcps <- mcp_95$area
data_summary$species <- min_time$species
data_summary
