ck_tanz_data <- filter(mydata, study == "ck_tanz")
ck_tanz_data
#' Check for duplicated observations (ones with same lat, long, timestamp,
#'  and individual identifier).
ind2<-ck_tanz_data %>% select(long, lat, id) %>%
duplicated
sum(ind2)
# remove them
ck_tanz_data$dups <- ind2
ck_tanz_data <- filter(ck_tanz_data,dups=="FALSE")
ck_tanz_data
# set the time column
levels(factor(ck_tanz_data$id))
# can look at an individual level with
(filter(ck_tanz_data,id=="33021"))
# all of the data is in the format of day-month-year
ck_tanz_data$New_time<-parse_date_time(x=ck_tanz_data$time,c("%d/%m/%Y %H:%M"))
# keep only the new time data
ck_tanz_data <- select(ck_tanz_data, New_time,long,lat,id,species,study)
ck_tanz_data <- rename(ck_tanz_data, time = New_time)
ck_tanz_data
# check the minimum time and the maximum time
min_time <- ck_tanz_data %>% group_by(id) %>% slice(which.min(time))
data.frame(min_time)
max_time <- ck_tanz_data %>% group_by(id) %>% slice(which.max(time))
data.frame(max_time)
#' filter extreme data based on a speed threshold
#' based on vmax which is km/hr
#' time needs to be labelled DateTime for these functions to work
names(ck_tanz_data)[names(ck_tanz_data) == 'time'] <- 'DateTime'
SDLfilterData<-ddfilter.speed(data.frame(ck_tanz_data), vmax = 60, method = 1)
length(SDLfilterData$DateTime)
#' rename everything as before
ck_tanz_data <- SDLfilterData
names(ck_tanz_data)[names(ck_tanz_data) == 'DateTime'] <- 'time'
# try the amt package
trk <- mk_track(ck_tanz_data, .x=long, .y=lat, .t=time, id = id, species=species,
crs = CRS("+init=epsg:4326"))
# Now it is easy to calculate day/night with either movement track
trk <- trk %>% time_of_day()
#' Save the class here (and apply it later after adding columns to the
#' object)
trk.class<-class(trk)
# nest by id
nesttrk<-trk%>%nest(-id)
nesttrk
#' We can add a columns to each nested column of data using purrr::map
trk<-trk %>% nest(-id) %>%
mutate(dir_abs = map(data, direction_abs,full_circle=TRUE, zero="N"),
dir_rel = map(data, direction_rel),
sl = map(data, step_lengths),
nsd_=map(data, nsd))%>%unnest()
#' Now, calculate month, year, hour, week of each observation and append these to the dataset
#' Unlike the movement charactersitics, these calculations can be done all at once,
#' since they do not utilize successive observations (like step lengths and turn angles do).
trk<-trk%>%
mutate(
week=week(t_),
month = month(t_, label=TRUE),
year=year(t_),
hour = hour(t_)
)
#' Now, we need to again tell R that this is a track (rather
#' than just a data frame)
class(trk)
class(trk)<-trk.class
#' Lets take a look at what we created
trk <- trk %>% group_by(id)
trk
#' look at net-squared displacement
ggplot(trk, aes(x = t_, y=nsd_)) + geom_point()+
facet_wrap(~id, scales="free")
#' some data points look a little off
#' we can identify them to investiage further and remove them
#' if needs be
filter(trk,id=="163115" & nsd_ < 15)
trk<- trk  %>%
filter(!((id=="163115" & nsd_ > 15)))
# select ck_tanz data which is Corinne's data from everything
ck_tanz_data <- filter(mydata, study == "ck_tanz")
ck_tanz_data
#' Check for duplicated observations (ones with same lat, long, timestamp,
#'  and individual identifier).
ind2<-ck_tanz_data %>% select(long, lat, id) %>%
duplicated
sum(ind2)
# remove them
ck_tanz_data$dups <- ind2
ck_tanz_data <- filter(ck_tanz_data,dups=="FALSE")
ck_tanz_data
library(lubridate)
library(SDLfilter)
library(amt)
library(sp)
# select ck_tanz data which is Corinne's data from everything
ck_tanz_data <- filter(mydata, study == "ck_tanz")
ck_tanz_data
#' Check for duplicated observations (ones with same lat, long, timestamp,
#'  and individual identifier).
ind2<-ck_tanz_data %>% select(long, lat, id) %>%
duplicated
library(tidyverse)
sum(ind2)
#' Check for duplicated observations (ones with same lat, long, timestamp,
#'  and individual identifier).
ind2<-ck_tanz_data %>% select(long, lat, id) %>%
duplicated
library(amt)
#########################################################################
#' Vulture comparative analysis
#' tutorials here https://www.jessesadler.com/post/gis-with-r-intro/
#' and here https://www.r-spatial.org/
#' 06 November 2018
#' 1_load_data - this loads in all of the tracking data and binds it
#########################################################################
#' Load the required packages
library(readr)
library(tidyverse)
#' Section 1: Load the data ----
data_path <- "raw_data"   # path to the data
files <- dir(data_path, pattern = "*.csv") # get file names
mydata <- files %>%
# read in all the files, appending the path before the filename
map(~ read_csv(file.path(data_path, .))) %>%
reduce(rbind)
mydata <- files %>%
# read in all the files, appending the path before the filename
map(~ read_csv(file.path(data_path, .))) %>%
reduce(rbind)
#########################################################################
#' Vulture comparative analysis
#' tutorials here https://www.jessesadler.com/post/gis-with-r-intro/
#' and here https://www.r-spatial.org/
#' 06 November 2018
#' 1_load_data - this loads in all of the tracking data and binds it
#########################################################################
#' Load the required packages
library(readr)
library(tidyverse)
#' Section 1: Load the data ----
data_path <- "raw_data"   # path to the data
files <- dir(data_path, pattern = "*.csv") # get file names
mydata <- files %>%
# read in all the files, appending the path before the filename
map(~ read_csv(file.path(data_path, .))) %>%
reduce(rbind)
#########################################################################
#' Vulture comparative analysis
#' tutorials here https://www.jessesadler.com/post/gis-with-r-intro/
#' and here https://www.r-spatial.org/
#' 06 November 2018
#' 1_load_data - this loads in all of the tracking data and binds it
#########################################################################
#' Load the required packages
library(readr)
library(tidyverse)
#' Section 1: Load the data ----
data_path <- "raw_data"   # path to the data
files <- dir(data_path, pattern = "*.csv") # get file names
mydata <- files %>%
# read in all the files, appending the path before the filename
map(~ read_csv(file.path(data_path, .))) %>%
reduce(rbind)
library(lubridate)
library(SDLfilter)
library(amt)
library(sp)
# select swazi data which is the data we tracked in Swaziland
swazi_data <- filter(mydata, study == "swazi")
swazi_data
#' Check for duplicated observations (ones with same lat, long, timestamp,
#'  and individual identifier).
ind2<-swazi_data %>% select(long, lat, id) %>%
duplicated
sum(ind2)
#' Check for duplicated observations (ones with same lat, long, timestamp,
#'  and individual identifier).
ind2<-swazi_data %>% select(timestamp, long, lat, id) %>%
duplicated
head(swazi_data)
#' Check for duplicated observations (ones with same lat, long, timestamp,
#'  and individual identifier).
ind2<-swazi_data %>% select(time, long, lat, id) %>%
duplicated
sum(ind2)
#########################################################################
#' Vulture comparative analysis
#' tutorials here https://www.jessesadler.com/post/gis-with-r-intro/
#' and here https://www.r-spatial.org/
#' 06 November 2018
#' 1_load_data - this loads in all of the tracking data and binds it
#########################################################################
#' Load the required packages
library(readr)
library(tidyverse)
#' Section 1: Load the data ----
data_path <- "raw_data"   # path to the data
files <- dir(data_path, pattern = "*.csv") # get file names
mydata <- files %>%
# read in all the files, appending the path before the filename
map(~ read_csv(file.path(data_path, .))) %>%
reduce(rbind)
#' filter the data to remove obvious outliers
mydata <- filter(mydata, lat < 20 & lat > -40 & long > 10)
head(mydata)
tail(mydata)
str(mydata)
levels(as.factor(mydata$study))
library(lubridate)
library(SDLfilter)
library(amt)
library(sp)
# select swazi data which is the data we tracked in Swaziland
swazi_data <- filter(mydata, study == "swazi")
swazi_data
#' Check for duplicated observations (ones with same lat, long, timestamp,
#'  and individual identifier).
ind2<-swazi_data %>% select(time, long, lat, id) %>%
duplicated
sum(ind2)
# remove them
swazi_data$dups <- ind2
swazi_data <- filter(swazi_data,dups=="FALSE")
swazi_data
# set the time column
levels(factor(swazi_data$id))
# can look at an individual level with
(filter(swazi_data,id=="ID1"))
# all of the data is in the format of day-month-year
swazi_data$New_time<-parse_date_time(x=swazi_data$time,c("%d/%m/%Y %H:%M"))
# keep only the new time data
swazi_data <- select(swazi_data, New_time,long,lat,id,species,study)
swazi_data <- rename(swazi_data, time = New_time)
swazi_data
# check the minimum time and the maximum time
min_time <- swazi_data %>% group_by(id) %>% slice(which.min(time))
data.frame(min_time)
max_time <- swazi_data %>% group_by(id) %>% slice(which.max(time))
data.frame(max_time)
#' filter extreme data based on a speed threshold
#' based on vmax which is km/hr
#' time needs to be labelled DateTime for these functions to work
names(swazi_data)[names(swazi_data) == 'time'] <- 'DateTime'
SDLfilterData<-ddfilter.speed(data.frame(swazi_data), vmax = 60, method = 1)
(40038/301986)*100
length(SDLfilterData$DateTime)
#' rename everything as before
swazi_data <- SDLfilterData
names(swazi_data)[names(swazi_data) == 'DateTime'] <- 'time'
# try the amt package
trk <- mk_track(swazi_data, .x=long, .y=lat, .t=time, id = id, species=species,
crs = CRS("+init=epsg:4326"))
# Now it is easy to calculate day/night with either movement track
trk <- trk %>% time_of_day()
#' Save the class here (and apply it later after adding columns to the
#' object)
trk.class<-class(trk)
# nest by id
nesttrk<-trk%>%nest(-id)
nesttrk
#' We can add a columns to each nested column of data using purrr::map
trk<-trk %>% nest(-id) %>%
mutate(dir_abs = map(data, direction_abs,full_circle=TRUE, zero="N"),
dir_rel = map(data, direction_rel),
sl = map(data, step_lengths),
nsd_=map(data, nsd))%>%unnest()
#' Now, calculate month, year, hour, week of each observation and append these to the dataset
#' Unlike the movement charactersitics, these calculations can be done all at once,
#' since they do not utilize successive observations (like step lengths and turn angles do).
trk<-trk%>%
mutate(
week=week(t_),
month = month(t_, label=TRUE),
year=year(t_),
hour = hour(t_)
)
#' Now, we need to again tell R that this is a track (rather
#' than just a data frame)
class(trk)
class(trk)<-trk.class
#' Lets take a look at what we created
trk <- trk %>% group_by(id)
trk
#' look at net-squared displacement
ggplot(trk, aes(x = t_, y=nsd_)) + geom_point()+
facet_wrap(~id, scales="free")
#' look at net-squared displacement
ggplot(trk, aes(x = t_, y = nsd_)) + geom_point() +
facet_wrap( ~ id, scales = "free")
#' export the plot
swazi_net_disp <- ggplot(trk, aes(x = t_, y=nsd_)) + geom_point()+
facet_wrap(~id, scales="free")
ggsave("plots/swazi_net_disp.pdf")
ggsave("plots/swazi_net_disp.png")
#' ### Absolute angles (for each movement) relative to North
#' We could use a rose diagram (below) to depict the distribution of angles.
#+fig.height=12, fig.width=12
ggplot(trk, aes(x = dir_abs, y = ..density..)) + geom_histogram(breaks = seq(0, 360, by =
20)) +
coord_polar(start = 0) + theme_minimal() +
scale_fill_brewer() + ylab("Density") + ggtitle("Angles Direct") +
scale_x_continuous(
"",
limits = c(0, 360),
breaks = seq(0, 360, by = 20),
labels = seq(0, 360, by = 20)
) +
facet_wrap( ~ id)
#' ### Turning angles
#'
#' Note: a 0 indicates the animal continued to move in a straight line, a 180
#' indicates the animal turned around (but note, resting + measurement error often can
#' make it look like the animal turned around).
#+fig.height=12, fig.width=12
ggplot(trk, aes(x = dir_rel, y = ..density..)) + geom_histogram(breaks = seq(-180, 180, by =
20)) +
coord_polar(start = 0) + theme_minimal() +
scale_fill_brewer() + ylab("Density") + ggtitle("Angles Direct") +
scale_x_continuous(
"",
limits = c(-180, 180),
breaks = seq(-180, 180, by = 20),
labels = seq(-180, 180, by = 20)
) +
facet_wrap(~ id)
#' Home range data
mcps.week <- trk %>%
mutate(year = year(t_),
month = month(t_),
week = week(t_)) %>%
group_by(id, year, month, week) %>% nest() %>%
mutate(mcparea = map(data, ~ hr_mcp(., levels = c(0.95)) %>% hr_area)) %>%  select(id, year, month, week, mcparea) %>% unnest()
#' home range plots
ggplot(mcps.week, aes(x = week, y = area, colour = factor(year))) +
geom_point() +
geom_smooth() + facet_wrap(~id, scales="free")
#' Kernel utilisation distributions
kde.week <- trk %>%
mutate(year = year(t_), month = month(t_), week = week(t_)) %>%
group_by(id, year, month, week) %>% nest() %>%
mutate(kdearea = map(data, ~ hr_kde(., levels=c(0.95)) %>% hr_area)) %>%
select(id, year, month, week, kdearea) %>% unnest()
ggplot(kde.week, aes(x = week, y = kdearea, colour = factor(year))) +
geom_point() +
geom_smooth() + facet_wrap(~id, scales = "free")
kde.week
#' plot the tracks
ggplot(swazi_data, aes(x = location_long,
y = location_lat)) + geom_point() +
facet_wrap(~local_identifier, scales = "free")
head(swazi_data)
#' plot the tracks
ggplot(swazi_data, aes(x = long,
y = lat)) + geom_point() +
facet_wrap(~id, scales = "free")
library(lubridate)
library(SDLfilter)
library(amt)
library(sp)
library(ggplot2)
library(moveVis)
library(move)
library(magrittr)
library(raster)
#' select swazi data which is the data we tracked in Swaziland
swazi_data <- filter(mydata, study == "swazi")
swazi_data
#########################################################################
#' Vulture comparative analysis
#' tutorials here https://www.jessesadler.com/post/gis-with-r-intro/
#' and here https://www.r-spatial.org/
#' 06 November 2018
#' 1_load_data - this loads in all of the tracking data and binds it
#########################################################################
#' Load the required packages
library(readr)
library(tidyverse)
#' Section 1: Load the data ----
data_path <- "raw_data"   # path to the data
files <- dir(data_path, pattern = "*.csv") # get file names
mydata <- files %>%
# read in all the files, appending the path before the filename
map(~ read_csv(file.path(data_path, .))) %>%
reduce(rbind)
#' filter the data to remove obvious outliers
mydata <- filter(mydata, lat < 20 & lat > -40 & long > 10)
head(mydata)
tail(mydata)
str(mydata)
levels(as.factor(mydata$study))
library(lubridate)
library(SDLfilter)
library(amt)
library(sp)
library(ggplot2)
library(moveVis)
library(move)
library(magrittr)
library(raster)
#' select swazi data which is the data we tracked in Swaziland
swazi_data <- filter(mydata, study == "swazi")
swazi_data
#' Check for duplicated observations (ones with same lat, long, timestamp,
#'  and individual identifier).
ind2 <- swazi_data %>% dplyr::select(time, long, lat, id) %>%
duplicated
sum(ind2)
# remove them
swazi_data$dups <- ind2
swazi_data <- filter(swazi_data, dups == "FALSE")
swazi_data
# set the time column
levels(factor(swazi_data$id))
# can look at an individual level with
(filter(swazi_data, id == "ID1"))
# all of the data is in the format of day-month-year
swazi_data$New_time <-
parse_date_time(x = swazi_data$time, c("%d/%m/%Y %H:%M"))
# keep only the new time data
swazi_data <-
dplyr::select(swazi_data, New_time, long, lat, id, species, study)
swazi_data <- rename(swazi_data, time = New_time)
swazi_data
# check the minimum time and the maximum time
min_time <- swazi_data %>% group_by(id) %>% slice(which.min(time))
data.frame(min_time)
max_time <- swazi_data %>% group_by(id) %>% slice(which.max(time))
data.frame(max_time)
#' filter extreme data based on a speed threshold
#' based on vmax which is km/hr
#' time needs to be labelled DateTime for these functions to work
names(swazi_data)[names(swazi_data) == 'time'] <- 'DateTime'
SDLfilterData <-
ddfilter.speed(data.frame(swazi_data), vmax = 60, method = 1)
length(SDLfilterData$DateTime)
#' rename everything as before
swazi_data <- SDLfilterData
names(swazi_data)[names(swazi_data) == 'DateTime'] <- 'time'
# try the amt package
trk <- mk_track(swazi_data, .x=long, .y=lat, .t=time, id = id, species=species,
crs = CRS("+init=epsg:4326"))
# Now it is easy to calculate day/night with either movement track
trk <- trk %>% time_of_day()
head(trk)
#' subset a sample of the data using head for an example
sample_data <- dplyr::filter(trk, id == "ID1" & tod_ == "day")
#' transform the data into a move object
sample_data <-
move(
x = sample_data$x_,
y = sample_data$y_,
time = sample_data$t_,
proj = CRS("+proj=longlat +ellps=WGS84")
)
#'
#' ### Make frames of tracking data for animation.
#' Evaluate tracking data for sampling rates if unknown. Use this information to help decide
#' the temporal resolution at which to align the data for the animation.
unique(timestamps(sample_data))
timeLag(sample_data, unit = "mins")
#' pick specific rows
sample_data <- slice(data.frame(sample_data), 235:435)
#' turn it back into a move object
sample_data <-
move(
x = sample_data$x,
y = sample_data$y,
time = sample_data$time,
proj = CRS("+proj=longlat +ellps=WGS84")
)
#' Align tracking data to uniform temporal resolution for interpretation by frames_spatial.
vultures <-
align_move(sample_data,
res = 1,
unit = "mins",
spaceMethod = "greatcircle")
frames <-
frames_spatial(
vultures,
map_service = "osm",
map_type = "watercolor",
equidistant = FALSE,
path_legend = T,
path_legend_title = "Vulture",
alpha = 0.5
)
#' Add labels and a progress bar.
frames.l <-
add_labels(frames, x = "Longitude", y = "Latitude") %>% # axis labels
add_progress() %>% # progress bar
add_scalebar() %>% # scale bar
add_northarrow() %>% # north arrow
add_timestamps(geese, type = "label") # timestamps
#' Add labels and a progress bar.
frames.l <-
add_labels(frames, x = "Longitude", y = "Latitude") %>% # axis labels
add_progress() %>% # progress bar
add_scalebar() %>% # scale bar
add_northarrow() %>% # north arrow
add_timestamps(vultures, type = "label") # timestamps
#' Record an animation using defaults shown in in moveVis manual p.18.
animate_frames(
frames.l,
"animations/swazi_vulture_test.gif",
fps = 25,
width = 500,
height = 800,
res = 100,
display = TRUE,
overwrite = TRUE,
verbose = TRUE
)
