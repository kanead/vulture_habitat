# read in all the files, appending the path before the filename
map(~ read_csv(file.path(data_path, .))) %>%
reduce(rbind)
res_data
#' associate a broad geographic location with each study
#' CK = east
#' masai = east
#' north = east
#' GA_Namibia = south
#' inter = south
#' Kerri = south
#' mend_Namibia = south
#' Morgan = south
#' Swazi = south
#' andre = south
#' ralph = south
res_data$region <- if_else(res_data$study == "CK" | res_data$study == "masai" | res_data$study == "north", "east", "south")
#'export the combined summary stats table for the temporal resolution of the data
write.csv(res_data, file="results/summary_all_tracks.csv", row.names = FALSE)
#' run some analyses on the data
#' extract data for birds that were tracked for over a year and remove some suspect data
#'the double releases from Kerri
nonWantedLevels<-c("AG382", "AM89", "AM88", "AM87")
subset <- res_data %>% filter(!id %in% nonWantedLevels)
levels(as.factor(subset$id))
#' how long were the birds tracked for?
p0 <- ggplot(data = subset, mapping = aes(x = species, y = duration, fill = factor(species))) + geom_boxplot(alpha = 0.5, show.legend = FALSE) + ylab("duration (days)") +
theme_bw()
p0 + theme(legend.position = "none")
#' extract just the gyps for all durations
gyps <- subset %>% filter(species == c("cv","rv","wb"))
#' extract just the gyps for all durations
species_to_keep <-c("cv","rv","wb")
gyps <- subset %>% filter(id %in% species_to_keep)
#' how long were the gyps tracked for
ggplot(data = gyps, mapping = aes(x = species, y = duration, fill = factor(species))) + geom_boxplot(alpha = 0.5, show.legend = FALSE) + ylab("duration (days)") +
theme_bw() + theme(legend.position = "none", axis.text=element_text(size=20),axis.title=element_text(size=20))
gyps <- subset %>% filter(species %in% species_to_keep)
#' how long were the gyps tracked for
ggplot(data = gyps, mapping = aes(x = species, y = duration, fill = factor(species))) + geom_boxplot(alpha = 0.5, show.legend = FALSE) + ylab("duration (days)") +
theme_bw() + theme(legend.position = "none", axis.text=element_text(size=20),axis.title=element_text(size=20))
#' keep only the birds that were tracked for over a year
year <- filter(subset, duration > 365)
#' boxplots of the KDEs by region
ggplot(data = year, mapping = aes(x = region, y = kde)) + geom_boxplot() + ylab("KDE 95%")
#' group by species and get the mean kde
year %>% group_by(species) %>% summarize(mean_kde = mean(kde))
#' group by species and get the max kde
year %>% group_by(species) %>% summarize(max_kde = max(kde))
#' boxplots of the KDEs by speices
ggplot(data = year, mapping = aes(x = species, y = kde)) + geom_boxplot() + ylab("KDE 95%")
#' group by species and get the median kde
year %>% group_by(species) %>% summarize(median_kde = median(kde))
#' extract one species and get its max kde
filter(year, species == "cv") %>% summarize(maxkde = max(kde))
#' group by region and get the median kde
year %>% group_by(region) %>% summarize(median_kde = median(kde))
#' subset to Gyps that were tracked for over a year
gyps_year <- year %>% filter(species == c("cv","rv","wb"))
p1 <- ggplot(data = gyps_year, mapping = aes(x = species, y = kde, fill = factor(species))) + geom_boxplot(alpha = 0.5, show.legend = FALSE) + ylab("KDE 95% km2") + theme_bw()
p1
#' remove outliers
# compute lower and upper whiskers
ylim1 = boxplot.stats(gyps_year$kde)$stats[c(1, 5)]
# scale y limits based on ylim1
p2 = p1 + coord_cartesian(ylim = ylim1*1.05) + theme(legend.position = "none", axis.text=element_text(size=20),axis.title=element_text(size=20))
p2
#' plot white backs by region
wb <- gyps_year %>% filter(species == c("wb"))
p3 <- ggplot(data = wb, mapping = aes(x = region, y = kde, fill = factor(region))) + geom_boxplot(alpha = 0.5, show.legend = FALSE) + ylab("KDE 95% km2") + theme_bw()
p3
#' remove outliers
# compute lower and upper whiskers
ylim1 = boxplot.stats(wb$kde)$stats[c(1, 5)]
# scale y limits based on ylim1
p4 = p3 + coord_cartesian(ylim = ylim1*2.5) + theme(legend.position = "none", axis.text=element_text(size=20),axis.title=element_text(size=20))
p4
gyps_year
year
filter(year, study == "swazi")
filter(year, study == "Swazi")
class(year$duration)
levels(year$species)
levels(as.factor(year$species))
gyps_year
year
tail(year)
#' subset to Gyps that were tracked for over a year
gyps_year <- year %>% dplyr::filter(species == c("cv","rv","wb"))
p1 <- ggplot(data = gyps_year, mapping = aes(x = species, y = kde, fill = factor(species))) + geom_boxplot(alpha = 0.5, show.legend = FALSE) + ylab("KDE 95% km2") + theme_bw()
p1
#' remove outliers
# compute lower and upper whiskers
ylim1 = boxplot.stats(gyps_year$kde)$stats[c(1, 5)]
# scale y limits based on ylim1
p2 = p1 + coord_cartesian(ylim = ylim1*1.05) + theme(legend.position = "none", axis.text=element_text(size=20),axis.title=element_text(size=20))
p2
#' plot white backs by region
wb <- gyps_year %>% filter(species == c("wb"))
p3 <- ggplot(data = wb, mapping = aes(x = region, y = kde, fill = factor(region))) + geom_boxplot(alpha = 0.5, show.legend = FALSE) + ylab("KDE 95% km2") + theme_bw()
p3
#' remove outliers
# compute lower and upper whiskers
ylim1 = boxplot.stats(wb$kde)$stats[c(1, 5)]
# scale y limits based on ylim1
p4 = p3 + coord_cartesian(ylim = ylim1*2.5) + theme(legend.position = "none", axis.text=element_text(size=20),axis.title=element_text(size=20))
p4
wb
#' plot white backs by region
wb <- year %>% filter(species == c("wb"))
p3 <- ggplot(data = wb, mapping = aes(x = region, y = kde, fill = factor(region))) + geom_boxplot(alpha = 0.5, show.legend = FALSE) + ylab("KDE 95% km2") + theme_bw()
p3
#' remove outliers
# compute lower and upper whiskers
ylim1 = boxplot.stats(wb$kde)$stats[c(1, 5)]
# scale y limits based on ylim1
p4 = p3 + coord_cartesian(ylim = ylim1*2.5) + theme(legend.position = "none", axis.text=element_text(size=20),axis.title=element_text(size=20))
p4
# scale y limits based on ylim1
p4 = p3 + coord_cartesian(ylim = ylim1*1.05) + theme(legend.position = "none", axis.text=element_text(size=20),axis.title=element_text(size=20))
p4
p4
wb
#' subset to Gyps that were tracked for over a year
gyps_year <- year %>% dplyr::filter(species == c("cv","rv","wb"))
p1 <- ggplot(data = gyps_year, mapping = aes(x = species, y = kde, fill = factor(species))) + geom_boxplot(alpha = 0.5, show.legend = FALSE) + ylab("KDE 95% km2") + theme_bw()
p1
#' remove outliers
# compute lower and upper whiskers
ylim1 = boxplot.stats(gyps_year$kde)$stats[c(1, 5)]
# scale y limits based on ylim1
p2 = p1 + coord_cartesian(ylim = ylim1*1.05) + theme(legend.position = "none", axis.text=element_text(size=20),axis.title=element_text(size=20))
p2
#' plot white backs by region
wb <- year %>% dplyr::filter(species == c("wb"))
p3 <- ggplot(data = wb, mapping = aes(x = region, y = kde, fill = factor(region))) + geom_boxplot(alpha = 0.5, show.legend = FALSE) + ylab("KDE 95% km2") + theme_bw()
p3
#' remove outliers
# compute lower and upper whiskers
ylim1 = boxplot.stats(wb$kde)$stats[c(1, 5)]
# scale y limits based on ylim1
p4 = p3 + coord_cartesian(ylim = ylim1*1.05) + theme(legend.position = "none", axis.text=element_text(size=20),axis.title=element_text(size=20))
p4
gyps <- subset %>% dplyr::filter(species %in% species_to_keep)
#' how long were the gyps tracked for
ggplot(data = gyps, mapping = aes(x = species, y = duration, fill = factor(species))) + geom_boxplot(alpha = 0.5, show.legend = FALSE) + ylab("duration (days)") +
theme_bw() + theme(legend.position = "none", axis.text=element_text(size=20),axis.title=element_text(size=20))
library(lubridate)
library(SDLfilter)
library(amt)
library(sp)
#########################################################################
#' Vulture comparative analysis
#' tutorials here https://www.jessesadler.com/post/gis-with-r-intro/
#' and here https://www.r-spatial.org/
#' 06 November 2018
#' 1_load_data - this loads in all of the tracking data and binds it
#########################################################################
#' Load the required packages
library(readr)
library(tidyverse)
#' Section 1: Load the data ----
data_path <- "raw_data"   # path to the data
files <- dir(data_path, pattern = "*.csv") # get file names
mydata <- files %>%
# read in all the files, appending the path before the filename
map(~ read_csv(file.path(data_path, .))) %>%
reduce(rbind)
#' filter the data to remove obvious outliers
mydata <- filter(mydata, lat < 20 & lat > -40 & long > 10 & long < 60)
head(mydata)
tail(mydata)
str(mydata)
levels(as.factor(mydata$study))
library(lubridate)
library(SDLfilter)
library(amt)
library(sp)
# select orr data which is Orr's data from everything
orr_data <- filter(mydata, study == "orr")
orr_data
#' Check for duplicated observations (ones with same lat, long, time,
#'  and individual identifier).
ind2 <- orr_data %>% select(long, lat, id) %>%
duplicated
sum(ind2)
# remove them
orr_data$dups <- ind2
orr_data <- filter(orr_data, dups == "FALSE")
orr_data
# set the time column
levels(factor(orr_data$id))
# can look at an individual level with
(filter(orr_data, id == "15"))
orr_data$New_time <-
parse_date_time(x = orr_data$time, c("%d/%m/%Y %H:%M"))
orr_data
# select orr data which is Orr's data from everything
orr_data <- filter(mydata, study == "orr")
orr_data
#' Check for duplicated observations (ones with same lat, long, time,
#'  and individual identifier).
ind2 <- orr_data %>% select(long, lat, id) %>%
duplicated
sum(ind2)
# remove them
orr_data$dups <- ind2
orr_data <- filter(orr_data, dups == "FALSE")
orr_data
# set the time column
levels(factor(orr_data$id))
# can look at an individual level with
(filter(orr_data, id == "15"))
tail(orr_data)
# Orr's Vulture Tracking Dataset
library(lubridate)
library(SDLfilter)
library(amt)
library(sp)
# orr
# select orr data which is Orr's data from everything
orr_data <- filter(mydata, study == "orr")
orr_data
#' Check for duplicated observations (ones with same lat, long, time,
#'  and individual identifier).
ind2 <- orr_data %>% select(long, lat, id) %>%
duplicated
sum(ind2)
# remove them
orr_data$dups <- ind2
orr_data <- filter(orr_data, dups == "FALSE")
orr_data
tail(orr_data)
# select orr data which is Orr's data from everything
orr_data <- filter(mydata, study == "orr")
orr_data
tail(orr_data)
#########################################################################
#' Vulture comparative analysis
#' tutorials here https://www.jessesadler.com/post/gis-with-r-intro/
#' and here https://www.r-spatial.org/
#' 06 November 2018
#' 1_load_data - this loads in all of the tracking data and binds it
#########################################################################
#' Load the required packages
library(readr)
library(tidyverse)
#' Section 1: Load the data ----
data_path <- "raw_data"   # path to the data
mydata <- files %>%
# read in all the files, appending the path before the filename
map(~ read_csv(file.path(data_path, .))) %>%
reduce(rbind)
files <- dir(data_path, pattern = "*.csv") # get file names
head(mydata)
#' filter the data to remove obvious outliers
mydata <- filter(mydata, lat < 20 & lat > -40 & long > 10 & long < 60)
tail(mydata)
#########################################################################
#' Vulture comparative analysis
#' tutorials here https://www.jessesadler.com/post/gis-with-r-intro/
#' and here https://www.r-spatial.org/
#' 06 November 2018
#' 1_load_data - this loads in all of the tracking data and binds it
#########################################################################
#' Load the required packages
library(readr)
library(tidyverse)
#' Section 1: Load the data ----
data_path <- "raw_data"   # path to the data
files <- dir(data_path, pattern = "*.csv") # get file names
mydata <- files %>%
# read in all the files, appending the path before the filename
map(~ read_csv(file.path(data_path, .))) %>%
reduce(rbind)
library(lubridate)
library(SDLfilter)
library(amt)
library(sp)
# select orr data which is Orr's data from everything
orr_data <- filter(mydata, study == "orr")
orr_data
#' Check for duplicated observations (ones with same lat, long, time,
#'  and individual identifier).
ind2 <- orr_data %>% select(long, lat, id) %>%
duplicated
sum(ind2)
# remove them
orr_data$dups <- ind2
orr_data <- filter(orr_data, dups == "FALSE")
orr_data
# select orr data which is Orr's data from everything
orr_data <- filter(mydata, study == "orr")
orr_data
tail(orr_data)
library(lubridate)
library(SDLfilter)
library(amt)
library(sp)
# select orr data which is Orr's data from everything
orr_data <- filter(mydata, study == "orr")
orr_data
head(mydata)
tail(mydata)
# select orr data which is Orr's data from everything
orr_data <- filter(mydata, study == "orr")
orr_data
as.Date(orr_data$time)
#########################################################################
#' Vulture comparative analysis
#' tutorials here https://www.jessesadler.com/post/gis-with-r-intro/
#' and here https://www.r-spatial.org/
#' 06 November 2018
#' 1_load_data - this loads in all of the tracking data and binds it
#########################################################################
#' Load the required packages
library(readr)
library(tidyverse)
#' Section 1: Load the data ----
data_path <- "raw_data"   # path to the data
files <- dir(data_path, pattern = "*.csv") # get file names
mydata <- files %>%
# read in all the files, appending the path before the filename
map(~ read_csv(file.path(data_path, .))) %>%
reduce(rbind)
#' filter the data to remove obvious outliers
mydata <- filter(mydata, lat < 20 & lat > -40 & long > 10 & long < 60)
head(mydata)
tail(mydata)
str(mydata)
levels(as.factor(mydata$study))
library(lubridate)
library(SDLfilter)
library(amt)
library(sp)
# select orr data which is Orr's data from everything
orr_data <- filter(mydata, study == "orr")
orr_data
#########################################################################
#' Vulture comparative analysis
#' tutorials here https://www.jessesadler.com/post/gis-with-r-intro/
#' and here https://www.r-spatial.org/
#' 06 November 2018
#' 1_load_data - this loads in all of the tracking data and binds it
#########################################################################
#' Load the required packages
library(readr)
library(tidyverse)
#' Section 1: Load the data ----
data_path <- "raw_data"   # path to the data
files <- dir(data_path, pattern = "*.csv") # get file names
mydata <- files %>%
# read in all the files, appending the path before the filename
map(~ read_csv(file.path(data_path, .))) %>%
reduce(rbind)
#' filter the data to remove obvious outliers
mydata <- filter(mydata, lat < 20 & lat > -40 & long > 10 & long < 60)
head(mydata)
tail(mydata)
str(mydata)
levels(as.factor(mydata$study))
library(lubridate)
library(SDLfilter)
library(amt)
library(sp)
# select orr data which is Orr's data from everything
orr_data <- filter(mydata, study == "orr")
orr_data
#########################################################################
#' Vulture comparative analysis
#' tutorials here https://www.jessesadler.com/post/gis-with-r-intro/
#' and here https://www.r-spatial.org/
#' 06 November 2018
#' 1_load_data - this loads in all of the tracking data and binds it
#########################################################################
#' Load the required packages
library(readr)
library(tidyverse)
#' Section 1: Load the data ----
data_path <- "raw_data"   # path to the data
files <- dir(data_path, pattern = "*.csv") # get file names
mydata <- files %>%
# read in all the files, appending the path before the filename
map(~ read_csv(file.path(data_path, .))) %>%
reduce(rbind)
#' filter the data to remove obvious outliers
mydata <- filter(mydata, lat < 20 & lat > -40 & long > 10 & long < 60)
head(mydata)
tail(mydata)
str(mydata)
levels(as.factor(mydata$study))
# Orr's Vulture Tracking Dataset
library(lubridate)
library(SDLfilter)
library(amt)
library(sp)
library(janitor)
# orr
# select orr data which is Orr's data from everything
orr_data <- filter(mydata, study == "orr")
orr_data
#' Check for duplicated observations (ones with same lat, long, time,
#'  and individual identifier).
ind2 <- orr_data %>% select(long, lat, id) %>%
duplicated
sum(ind2)
# remove them
orr_data$dups <- ind2
orr_data <- filter(orr_data, dups == "FALSE")
orr_data
tail(orr_data)
# set the time column
levels(factor(orr_data$id))
# can look at an individual level with
(filter(orr_data, id == "15"))
#' all of the data is in the format of day-month-year
#' time zone is UTC by default
orr_data$New_time <-
parse_date_time(x = orr_data$time, c("%d/%m/%Y %H:%M"))
# Orr's Vulture Tracking Dataset
library(lubridate)
library(SDLfilter)
library(amt)
library(sp)
library(janitor)
# orr
# select orr data which is Orr's data from everything
orr_data <- filter(mydata, study == "orr")
orr_data
#' Check for duplicated observations (ones with same lat, long, time,
#'  and individual identifier).
ind2 <- orr_data %>% select(long, lat, id) %>%
duplicated
sum(ind2)
# remove them
orr_data$dups <- ind2
orr_data <- filter(orr_data, dups == "FALSE")
orr_data
tail(orr_data)
# set the time column
levels(factor(orr_data$id))
# can look at an individual level with
(filter(orr_data, id == "15"))
#' all of the data is in the format of day-month-year
#' time zone is UTC by default
orr_data$New_time <-
parse_date_time(x = orr_data$time, c("%Y/%m/%d %H:%M:%S"))
orr_data
# keep only the new time data
orr_data <-
select(orr_data, New_time, long, lat, id, species, study)
orr_data <- rename(orr_data, time = New_time)
orr_data
#' filter extreme data based on a speed threshold
#' based on vmax which is km/hr
#' time needs to be labelled DateTime for these functions to work
names(orr_data)[names(orr_data) == 'time'] <- 'DateTime'
SDLfilterData <-
ddfilter.speed(data.frame(orr_data), vmax = 70, method = 1)
length(SDLfilterData$DateTime)
#' rename everything as before
orr_data <- SDLfilterData
names(orr_data)[names(orr_data) == 'DateTime'] <- 'time'
# check the minimum time and the maximum time
min_time <- orr_data %>% group_by(id) %>% slice(which.min(time))
data.frame(min_time)
max_time <- orr_data %>% group_by(id) %>% slice(which.max(time))
data.frame(max_time)
#' determine the length of time each bird was tracked for
duration <-difftime(max_time$time, min_time$time, units = "days"); duration
# try the amt package
trk <-
mk_track(
orr_data,
.x = long,
.y = lat,
.t = time,
id = id,
species = species,
crs = CRS("+init=epsg:4326")
) %>%
transform_coords(
sp::CRS(
#' we can transform the CRS of the data to an equal area projection
"+proj=aea +lat_1=20 +lat_2=-23 +lat_0=0 +lon_0=25 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs"
)
)
#' summarise the sampling rate
data_summary <- trk %>% nest(-id) %>% mutate(sr = map(data, summarize_sampling_rate)) %>%
dplyr::select(id, sr) %>% unnest %>% arrange(id)
#' Calculate home range size for data that is not regularised
mcps <- trk %>% nest(-id) %>%
mutate(mcparea = map(data, ~ hr_mcp(., levels = c(0.95)) %>% hr_area)) %>%
select(id, mcparea) %>% unnest()
mcps$area <- mcps$area / 1000000
mcp_95 <- mcps %>% arrange(id)
#' Same for KDE
kde <- trk %>% nest(-id) %>%
mutate(kdearea = map(data, ~ hr_kde(., levels = c(0.95)) %>% hr_area)) %>%
select(id, kdearea) %>% unnest()
kde$kdearea <-  kde$kdearea / 1000000
kde_95 <- kde %>% arrange(id)
#' combine the summary stats
data_summary$duration <- duration
data_summary$min_time <- min_time$time
data_summary$max_time <- max_time$time
data_summary$kde <- kde_95$kdearea
data_summary$mcps <- mcp_95$area
data_summary$species <- min_time$species
data_summary$study <- "orr"
data_summary
write.csv(data_summary, file="track_resolution_summary/orr_data_summary.csv", row.names = FALSE)
